# -*- coding: utf-8 -*-
"""
RakutenÁ´∂È¶¨ Áõ£Ë¶ñ„ÉªÈÄöÁü•„Éê„ÉÉ„ÉÅÔºàÂÆåÂÖ®Â∑Æ„ÅóÊõø„ÅàÁâà v2025-08-15BÔºâ
- Á∑†ÂàáÊôÇÂàªÔºöÂçòË§á„Ç™„ÉÉ„Ç∫/‰∏ÄË¶ß„Éö„Éº„Ç∏„Åã„Çâ‚ÄúÁ∑†Âàá‚Äù„ÇíÁõ¥Êé•ÊäΩÂá∫ÔºàÊúÄÂÑ™ÂÖàÔºâ
- Áô∫Ëµ∞ÊôÇÂàªÔºö‰∏ÄË¶ß„Éö„Éº„Ç∏ÂÑ™ÂÖàÔºã„Ç™„ÉÉ„Ç∫Ë©≥Á¥∞„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
- Á™ìÂà§ÂÆöÔºö„Çø„Éº„Ç≤„ÉÉ„ÉàÊôÇÂàªÔºàÁ∑†Âàá or Áô∫Ëµ∞ÔºâÂü∫Ê∫ñ„ÄÅ¬±GRACE_SECONDS „ÅÆË®±ÂÆπ
- ÈÄöÁü•ÔºöÁ™ìÂÜÖ1Âõû / 429ÊôÇ„ÅØ„ÇØ„Éº„É´„ÉÄ„Ç¶„É≥ / Google Sheet„ÅßTTLÊ∞∏Á∂ö
- ÈÄÅ‰ø°ÂÖàÔºöGoogle„Ç∑„Éº„Éà(„Çø„ÉñA=ÂêçÁß∞„Äå1„Äç)„ÅÆHÂàó„Åã„Çâ userId „ÇíÂèéÈõÜ
- Êà¶Áï•‚ë¢ÔºöÂ∞ÇÁî®„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÔºà1Ëª∏„ÉªÁõ∏Êâã10„Äú20ÂÄç„ÉªÂÄôË£úÊúÄÂ§ß4È†≠„ÉªÁÇπÊï∞Ë°®Á§∫Ôºâ
- È®éÊâã„É©„É≥„ÇØÔºöÂÜÖËîµ200‰ΩçÔºãË°®Ë®ò„ÇÜ„ÇåËÄêÊÄßÔºàÂº∑ÂåñÁâà„ÇØ„É¨„É≥„Ç∏„É≥„Ç∞ÔºãÂâçÊñπ‰∏ÄËá¥Ôºâ
- ÈÄöÁü•Êú¨ÊñáÔºö„ÄéÂçòÂãù‰∫∫Ê∞óÔºçÈ®éÊâã„É©„É≥„ÇØ„ÄèÔºà‰æãÔºö1ÔºçA-3ÔºçB-5ÔºçCÔºâ
- Êú™‰∏ÄËá¥„ÅÆÈ®éÊâãÂêç„ÅØ [RANKMISS] „É≠„Ç∞„Å´Ë®òÈå≤ÔºàÈáçË§áÊäëÊ≠¢Ôºâ
- bets„Ç∑„Éº„ÉàÔºöÈ¶¨Áï™„Éô„Éº„Çπ„ÅßË®òÈå≤
- Êó•Ê¨°„Çµ„Éû„É™ÔºöÊåáÂÆöÊôÇÂàª„Å´1Êó•1ÂõûÈÄÅ‰ø°Ôºà0‰ª∂„Åß„ÇÇÂèØÔºâ
- Âà∏Á®Æ„ÅØ STRATEGY_BET_KIND_JSON „ÅßË®≠ÂÆöÔºàÊó¢ÂÆö: ‚ë†È¶¨ÈÄ£, ‚ë°È¶¨Âçò, ‚ë¢‰∏âÈÄ£Âçò, ‚ë£‰∏âÈÄ£Ë§áÔºâ

‚òÖÁ∑†ÂàáÂü∫Ê∫ñ„ÅßÈÅãÁî®„Åô„ÇãÂ†¥ÂêàÔºö
  - Áí∞Â¢ÉÂ§âÊï∞ CUTOFF_OFFSET_MIN „Çí 5ÔºàÊé®Â•®Ôºâ„Å´Ë®≠ÂÆö
  - Êú¨Áâà„ÅØ ‚ÄúÁ∑†Âàá„Åù„ÅÆ„ÇÇ„ÅÆ‚Äù „ÇíÊäΩÂá∫„Åß„Åç„Åü„Çâ„Åù„Çå„ÇíÊé°Áî®„ÄÇÂèñ„Çå„Å™„ÅÑÂ†¥Âêà„ÅÆ„Åø„ÄåÁô∫Ëµ∞-5ÂàÜ„Äç„Çí‰ª£Áî®„ÄÇ
"""

import os, re, json, time, random, logging, pathlib, hashlib, unicodedata
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Tuple, Set

import requests
from bs4 import BeautifulSoup, Tag
from strategy_rules import eval_strategy

# --- ÈÄöÁü•„É≠„Ç∞ append „ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ‰ªò„Åç import ---
try:
    from utils_notify_log import append_notify_log
except ModuleNotFoundError:
    import logging as _logging
    def append_notify_log(*args, **kwargs):
        _logging.warning("[WARN] utils_notify_log „ÅåË¶ã„Å§„Åã„Çâ„Å™„ÅÑ„Åü„ÇÅ„ÄÅÈÄöÁü•„É≠„Ç∞„ÅÆËøΩË®ò„Çí„Çπ„Ç≠„ÉÉ„Éó„Åó„Åæ„Åô„ÄÇ")

# Êó•‰ªò„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£
from utils_summary import jst_today_str, jst_now

# ===== Google Sheets =====
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build

# ========= Âü∫Êú¨Ë®≠ÂÆö =========
JST = timezone(timedelta(hours=9))
SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/124.0.0.0 Safari/537.36"),
    "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
    "Cache-Control": "no-cache",
})
TIMEOUT = (10, 25)
RETRY = 3
SLEEP_BETWEEN = (0.6, 1.2)

LINE_PUSH_URL = "https://api.line.me/v2/bot/message/push"

# ========= Áí∞Â¢ÉÂ§âÊï∞ =========
START_HOUR          = int(os.getenv("START_HOUR", "10"))
END_HOUR            = int(os.getenv("END_HOUR",   "22"))
DRY_RUN             = os.getenv("DRY_RUN", "False").lower() == "true"
KILL_SWITCH         = os.getenv("KILL_SWITCH", "False").lower() == "true"
NOTIFY_ENABLED      = os.getenv("NOTIFY_ENABLED", "1") == "1"
DEBUG_RACEIDS       = [s.strip() for s in os.getenv("DEBUG_RACEIDS", "").split(",") if s.strip()]

NOTIFY_TTL_SEC      = int(os.getenv("NOTIFY_TTL_SEC", "3600"))
NOTIFY_COOLDOWN_SEC = int(os.getenv("NOTIFY_COOLDOWN_SEC", "1800"))

WINDOW_BEFORE_MIN   = int(os.getenv("WINDOW_BEFORE_MIN", "15"))
WINDOW_AFTER_MIN    = int(os.getenv("WINDOW_AFTER_MIN", "0"))   # Á∑†ÂàáÈÅãÁî®„Å™„Çâ 0 Êé®Â•®
CUTOFF_OFFSET_MIN   = int(os.getenv("CUTOFF_OFFSET_MIN", "0"))  # ‰æã: 5ÔºàÁ∑†ÂàáÁõ¥Êé•ÂèñÂæó„Åß„Åç„Å™„ÅÑÂ†¥Âêà„ÅÆ‰ª£Áî®Ôºâ
FORCE_RUN           = os.getenv("FORCE_RUN", "0") == "1"
GRACE_SECONDS       = int(os.getenv("GRACE_SECONDS", "60"))     # Â¢ÉÁïåË®±ÂÆπÔºàÁßíÔºâ

LINE_ACCESS_TOKEN   = os.getenv("LINE_ACCESS_TOKEN", "")
LINE_USER_ID        = os.getenv("LINE_USER_ID", "")
LINE_USER_IDS       = [s.strip() for s in os.getenv("LINE_USER_IDS", "").split(",") if s.strip()]

GOOGLE_CREDENTIALS_JSON = os.getenv("GOOGLE_CREDENTIALS_JSON", "")
GOOGLE_SHEET_ID         = os.getenv("GOOGLE_SHEET_ID", "")

# TTLÁÆ°ÁêÜ„Çø„ÉñÔºàÂêçÂâç or gidÔºâ
GOOGLE_SHEET_TAB        = os.getenv("GOOGLE_SHEET_TAB", "notified")

# ÈÄÅ‰ø°ÂÖà„É¶„Éº„Ç∂„Éº„ÇíË™≠„ÇÄ„Çø„ÉñAÔºà=„Äå1„ÄçÔºâ„Å®ÂàóÔºà=HÔºâ
USERS_SHEET_NAME        = os.getenv("USERS_SHEET_NAME", "1")
USERS_USERID_COL        = os.getenv("USERS_USERID_COL", "H")

# „Éô„ÉÉ„ÉàË®òÈå≤„Çø„Éñ
BETS_SHEET_TAB          = os.getenv("BETS_SHEET_TAB", "bets")

# Âà∏Á®ÆÔºàÊà¶Áï•‚ÜíÂà∏Á®ÆÔºâ
_DEFAULT_BET_KIND = {"1":"È¶¨ÈÄ£", "2":"È¶¨Âçò", "3":"‰∏âÈÄ£Âçò", "4":"‰∏âÈÄ£Ë§á"}
try:
    STRATEGY_BET_KIND = json.loads(os.getenv("STRATEGY_BET_KIND_JSON","")) or _DEFAULT_BET_KIND
except Exception:
    STRATEGY_BET_KIND = _DEFAULT_BET_KIND

UNIT_STAKE_YEN = int(os.getenv("UNIT_STAKE_YEN", "100"))  # 1ÁÇπ100ÂÜÜ

# === Êó•Ê¨°„Çµ„Éû„É™„ÅÆÊñ∞Ë¶èË®≠ÂÆö ===
DAILY_SUMMARY_HHMM = os.getenv("DAILY_SUMMARY_HHMM", "21:02")  # Ê±∫„Åæ„Å£„ÅüÊôÇÂàª„Å´1ÂõûÈÄÅ„Çã
ALWAYS_NOTIFY_DAILY_SUMMARY = os.getenv("ALWAYS_NOTIFY_DAILY_SUMMARY", "1") == "1"  # 0‰ª∂„Åß„ÇÇÈÄÅ„Çã

RACEID_RE   = re.compile(r"/RACEID/(\d{18})")
TIME_PATS = [
    re.compile(r"\b(\d{1,2}):(\d{2})\b"),
    re.compile(r"\b(\d{1,2})Ôºö(\d{2})\b"),
    re.compile(r"\b(\d{1,2})\s*ÊôÇ\s*(\d{1,2})\s*ÂàÜ\b"),
]
PLACEHOLDER = re.compile(r"\d{8}0000000000$")

# „É©„Éô„É´È°û
IGNORE_NEAR_PAT   = re.compile(r"(ÁèæÂú®|Êõ¥Êñ∞|Áô∫Â£≤|Á¢∫ÂÆö|ÊâïÊàª|ÂÆüÊ≥Å)")
POST_LABEL_PAT    = re.compile(r"(Áô∫Ëµ∞|Áô∫Ëµ∞‰∫àÂÆö|Áô∫Ëµ∞ÊôÇÂàª|Áô∫ÈÄÅ|Âá∫Ëµ∞)")
CUTOFF_LABEL_PAT  = re.compile(r"(ÊäïÁ•®Á∑†Âàá|Áô∫Â£≤Á∑†Âàá|Á∑†Âàá)")

# ========= È®éÊâã„É©„É≥„ÇØÔºà1„Äú200‰Ωç„ÇíÂÜÖËîµÔºâ =========
# RANKMISS ÈáçË§áÊäëÊ≠¢Áî®
_RANKMISS_SEEN: Set[str] = set()

def _log_rank_miss(orig: str, norm: str):
    key = f"{orig}|{norm}"
    if key not in _RANKMISS_SEEN:
        _RANKMISS_SEEN.add(key)
        logging.info("[RANKMISS] name_raw=%s name_norm=%s", orig, norm)

def _normalize_name(s: str) -> str:
    """Âº∑ÂåñÁâàÔºöÂÖ®ÂçäËßíÊ≠£Ë¶èÂåñ„ÉªÁ©∫ÁôΩÈô§Âéª„ÉªÊóßÂ≠ó‰Ωì/Áï∞‰ΩìÂ≠ó/‰∏ÄËà¨ÁöÑË™§Ë®ò„ÅÆ‰ª£Ë°®Ë°®Ë®òÂåñ"""
    if not s:
        return ""
    s = unicodedata.normalize("NFKC", s)
    s = s.replace(" ", "").replace("\u3000", "")

    # ‰ª£Ë°®Ë°®Ë®òÁΩÆÊèõÔºàÊã°ÂÖÖÔºâ
    replace_map = {
        # ÊóßÂ≠ó‰Ωì„ÉªÁï∞‰ΩìÂ≠ó
        "†Æ∑": "Âêâ", "Ê†Å": "Êü≥", "È´ô": "È´ò", "Êøµ": "Êµú", "Ô®ë": "Â¥é", "Â∂ã": "Â≥∂", "Â≥Ø": "Â≥∞",
        "ÈΩã": "Êñé", "ÈΩä": "Êñâ", "ÂÖß": "ÂÜÖ", "ÂÜ®": "ÂØå", "Âúã": "ÂõΩ", "È´î": "‰Ωì", "Áúû": "Áúü",
        "Âª£": "Â∫É",
        # ÈÇä„ÅÆÁï∞‰ΩìÂ≠ó
        "ÈÇä": "Ëæ∫", "ÈÇâ": "Ëæ∫",
        # Ê∏°ÈÇä/Ê∏°ÈÇâ ‚Üí Ê∏°Ëæ∫
        "Ê∏°ÈÇä": "Ê∏°Ëæ∫", "Ê∏°ÈÇâ": "Ê∏°Ëæ∫",
        # ‰øÉÈü≥„ÉªÈï∑Èü≥„ÅÆÊè∫„Çå„ÅØ„Åù„ÅÆ„Åæ„ÅæÔºàÊ∞èÂêç„ÅßÂ∞ë„Å™„ÅÑ„Åü„ÇÅÔºâ
    }
    for k, v in replace_map.items():
        s = s.replace(k, v)
    return s

def _clean_jockey_name(s: str) -> str:
    """Êã¨Âºß„ÉªÊñ§Èáè„ÉªÂç∞„Å™„Å©„ÇíÈô§Âéª„Åó„Å¶Á¥†„ÅÆÊ∞èÂêç„Å†„Åë„Å´„Åô„Çã"""
    if not s:
        return ""
    s = re.sub(r"[Ôºà(].*?[Ôºâ)]", "", s)                           # Êã¨ÂºßÂÜÖ
    s = re.sub(r"[‚ñ≤‚ñ≥‚òÜ‚òÖ‚óá‚óÜ‚äô‚óé‚óã‚óØ‚óâ‚ö™Ô∏éÔºã+Ôºä*]", "", s)                # Âç∞
    s = re.sub(r"\d+(?:\.\d+)?\s*(?:kg|Êñ§)?", "", s)               # Êñ§Èáè
    s = s.replace("Êñ§Èáè", "")
    s = s.replace("È®éÊâã", "").replace("J", "").replace("Ôº™", "")   # Êé•Â∞æËæû
    s = re.sub(r"\s+", "", s)
    return s

# 1„Äú200‰Ωç„É©„É≥„ÇØË°®Ôºà„ÅîÊèê‰æõ„ÅÆ„ÉÜ„Éº„Éñ„É´„ÇíË∏èË•≤Ôºâ
JOCKEY_RANK_TABLE_RAW: Dict[int, str] = {
    1:"Á¨πÂ∑ùÁøº",2:"Áü¢ÈáéË≤¥‰πã",3:"Â°öÊú¨ÂæÅÂêæ",4:"Â∞èÁâßÂ§™",5:"Â±±Êú¨ËÅ°Âìâ",6:"ÈáéÁïëÂáå",7:"Áü≥Â∑ùÂÄ≠",8:"Ê∞∏Ê£ÆÂ§ßÊô∫",9:"‰∏≠Â≥∂Èæç‰πü",10:"ÂêâÂéüÂØõ‰∫∫",
    11:"Â∫ÉÁÄ¨Ëà™",12:"Âä†Ëó§ËÅ°‰∏Ä",13:"ÊúõÊúàÊ¥µËºù",14:"Èà¥Êú®ÊÅµ‰ªã",15:"Ê∏°Ëæ∫Á´ú‰πü",16:"ËêΩÂêàÁéÑÂ§™",17:"Â±±Âè£Âã≤",18:"Êú¨Áî∞Ê≠£Èáç",19:"ÂêâÊùëÊô∫Ê¥ã",20:"Ëµ§Â≤°‰øÆÊ¨°",
    21:"Â≤°ÈÉ®Ë™†",22:"È´òÊùæ‰∫Æ",23:"È£õÁî∞ÊÑõÊñó",24:"Ë•øÂ∞ÜÂ§™",25:"Âæ°Á•ûÊú¨Ë®ìÂè≤",26:"‰∏ãÂéüÁêÜ",27:"Â±±Êú¨ÊîøËÅ°",28:"‰ªä‰∫ïË≤¥Â§ß",29:"Á≠í‰∫ïÂãá‰ªã",30:"Â±±Áî∞Áæ©Ë≤¥",
    31:"‰∏∏ÈáéÂãùËôé",32:"ÈùíÊü≥Ê≠£Áæ©",33:"Ê∏°Êù•ÂøÉË∑Ø",34:"‰ªä‰∫ïÂçÉÂ∞ã",35:"ÂíåÁî∞Ë≠≤Ê≤ª",36:"‰∫ï‰∏äÁëõÂ§™",37:"Â§öÁî∞ÁæÖË™†‰πü",38:"ÈáëÁî∞Âà©Ë≤¥",39:"Â°öÊú¨Ê∂º‰∫∫",40:"ÂÆÆ‰∏ãÁû≥",
    41:"Ê†óÂéüÂ§ßÊ≤≥",42:"Ë•øË¨ô‰∏Ä",43:"Ë•øÂïìÂ§™",44:"Èï∑Êæ§Âπ∏Â§™",45:"Â±±‰∏≠ÊÇ†Â∏å",46:"ËèäÊ±†‰∏ÄÊ®π",47:"Áî∫Áî∞Áõ¥Â∏å",48:"Áü≥Â∑ùÊÖéÂ∞Ü",49:"ËèÖÂéüËæ∞Âæ≥",50:"Â≥∂Ê¥•Êñ∞",
    51:"ÈòøÈÉ®Èæç",52:"Â∞èÈáéÊ•ìÈ¶¨",53:"Ëµ§Â°öÂÅ•‰ªÅ",54:"Âä†Ëó§ÁøîÈ¶¨",55:"ÊùâÊµ¶ÂÅ•Â§™",56:"ÂºµÁî∞ÊòÇ",57:"Ê°ëÊùëÁúüÊòé",58:"Â±±Êú¨ËÅ°Á¥Ä",59:"Âêâ‰∫ïÁ´†",60:"Â§ßÁïëÊÖßÊÇü",
    61:"Êü¥Áî∞ÂãáÁúü",62:"Â§ßÁïëÈõÖÁ´†",63:"Á¨πÁî∞Áü•ÂÆè",64:"Á¥∞Â∑ùÊô∫Âè≤",65:"ÈáëÂ±±ÊòáÈ¶¨",66:"Â≤©Êú¨ÊÄú",67:"Â≤°ÈÅºÂ§™ÈÉé",68:"Â≤°ÊùëÂçìÂº•",69:"‰∏≠ÂéüËìÆ",70:"Ëó§Êú¨Âå†",
    71:"È´òÊ©ãÊÇ†Èáå",72:"ÂúüÊñπÈ¢ØÂ§™",73:"Èï∑Ë∞∑ÈÉ®ÈßøÂº•",74:"È´òÊ©ãÊÑõÂè∂",75:"ÂèäÂ∑ùË£ï‰∏Ä",76:"Âä†ËåÇÈ£õÁøî",77:"Â∑ùÂéüÊ≠£‰∏Ä",78:"Êùë‰∏äÂøç",79:"Â≤°ÊùëÂÅ•Âè∏",80:"Áî∞ÈáéË±ä‰∏â",
    81:"Êùë‰∏äÂºòÊ®π",82:"Â±±Â¥éË™†Â£´",83:"Á´πÂêâÂæπ",84:"ÂÆÆÂÜÖÂãáÊ®π",85:"ËàπÂ±±Ëîµ‰∫∫",86:"‰∏≠ÊùëÂ§™ÈôΩ",87:"Êú¨Ê©ãÂ≠ùÂ§™",88:"Âá∫Ê∞¥Êãì‰∫∫",89:"Êñ∞Â∫ÑÊµ∑Ë™†",90:"Â±±Â¥éÈõÖÁî±",
    91:"ÈòøÈÉ®Ê≠¶Ëá£",92:"ÂÆâËó§Ê¥ã‰∏Ä",93:"Â∞èÊûóÂáå",94:"ÂèãÊ£ÆÁøîÂ§™ÈÉé",95:"Á¶èÂéüÊùè",96:"Â≤©Ê©ãÂãá‰∫å",97:"‰Ωê„ÄÖÊú®ÂøóÈü≥",98:"Êú®‰πãÂâçËëµ",99:"Ëó§Áî∞Âáå",100:"‰ΩêÈáéÈÅ•‰πÖ",
    101:"‰∫ï‰∏äÂππÂ§™",102:"‰ΩêËó§ÂèãÂâá",103:"ÂêâÊùëË™†‰πãÂä©",104:"ÂêâÊú¨ÈöÜË®ò",105:"Ê∏°Ëæ∫Á´ú‰πü",106:"Âêâ‰∫ïÂèãÂΩ¶",107:"Â≤°Áî∞Á••Âó£",108:"ÊùæÊú®Â§ßÂú∞",109:"Âä†Ëó§ÂíåÁæ©",110:"Áî∞‰∏≠Â≠¶",
    111:"Â∑ùÂ≥∂Êãì",112:"Ê£ÆÊ≥∞Êñó",113:"ÊúçÈÉ®ËåÇÂè≤",114:"Âä†Ëó§Ë™ì‰∫å",115:"Êø±Â∞öÁæé",116:"Ê∞∏‰∫ïÂ≠ùÂÖ∏",117:"È´òÈáéË™†ÊØÖ",118:"Â§ßÁïëÈõÖÁ´†",119:"Â§ßÂ±±ÁúüÂêæ",120:"Èï∑Ë∞∑ÈÉ®ÈßøÂº•",
    121:"‰∏πÁæΩÂÖãËºù",122:"Â±±Âè£Âã≤‰∫å",123:"Áî∞‰∏≠Â≠¶ËâØ",124:"ËêΩÂêàÁéÑÂ§™Êúó",125:"Á¥∞Â∑ùÊô∫Âè≤Êúó",126:"ÊùæÊú¨ÂâõÂè≤",127:"Ëó§ÂéüËâØ‰∏Ä",128:"Â±±Êú¨ÊîøËÅ°ËâØ",129:"‰ΩêÂéüÁßÄÊ≥∞",130:"Ëó§Áî∞ÂºòÊ≤ª",
    131:"ÂêâÁî∞ÊôÉÊµ©",132:"Â≤°ÊùëÂçìÂº•ËâØ",133:"ÂÆÆÂ∑ùÂÆü",134:"ÈÉ∑ÈñìÂãáÂ§™",135:"‰∏äÁî∞Â∞ÜÂè∏",136:"ÂÄâÂÖºËÇ≤Â∫∑",137:"Ëµ§Â≤°‰øÆ‰∫å",138:"ÊûóË¨ô‰Ωë",139:"Â§öÁî∞ÁæÖË™†‰πüËâØ",140:"Êø±Áî∞ÈÅî‰πü",
    141:"Áïë‰∏≠‰ø°Âè∏",142:"Â°öÊú¨ÈõÑÂ§ß",143:"Â≤°ÈÅºÂ§™ÈÉéËâØ",144:"Â≤©Êú¨ÊÄúËâØ",145:"Â§ßÂ±±ÈæçÂ§™ÈÉé",146:"‰Ωê„ÄÖÊú®ÂõΩÊòé",147:"Ê±†Ë∞∑Âå†Áøî",148:"‰Ωê„ÄÖÊú®‰∏ñÈ∫ó",149:"Â±±Áî∞ÈõÑÂ§ß",150:"Áî∞‰∏≠Â≠¶Â§ß",
    151:"‰∏≠Ë∂äÁêâ‰∏ñ",152:"Êø±Áî∞ÈÅî‰πüËâØ",153:"Â§ß‰πÖ‰øùÂèãÈõÖ",154:"Â∞èË∞∑Âë®Âπ≥",155:"Â§ßÊüø‰∏ÄÁúü",156:"Èï∑Ë∞∑ÈÉ®Èßø‰πü",157:"Áî∞ÊùëÁõ¥‰πü",158:"Áü≥Â†ÇÈüø",159:"Á´πÊùëÈÅî‰πü",160:"È¥®ÂÆÆÁ••Ë°å",
    161:"ÊùâÊµ¶ÂÅ•Â§™ËâØ",162:"‰∏ãÂéüÁêÜËâØ",163:"Áî∞‰∏≠Ê¥∏Â§ö",164:"Èï∑Áî∞ÈÄ≤‰ªÅ",165:"Â§ßÂ±±ÁúüÂêæËâØ",166:"Ê∏°Ëæ∫Ëñ´ÂΩ¶",167:"Â≤°Áî∞Á••Âó£ËâØ",168:"Âêâ‰∫ïÁ´†ËâØ",169:"ÊùæÊú®Â§ßÂú∞ËâØ",170:"Á¨πÁî∞Áü•ÂÆèËâØ",
    171:"‰∫ï‰∏äÁëõÂ§™ËâØ",172:"Âª£ÁÄ¨Ëà™",173:"Áî∞ÊùëÁõ¥‰πüËâØ",174:"Áü≥Â†ÇÈüøËâØ",175:"Â∞èË∞∑Âë®Âπ≥ËâØ",176:"‰∏≠Áî∞Ë≤¥Â£´",177:"Â§ßÊüø‰∏ÄÁúüËâØ",178:"Áî∞‰∏≠Â≠¶ÈöÜ",179:"Ê∞∏‰∫ïÂ≠ùÂÖ∏ËâØ",180:"ÊùâÊµ¶ÂÅ•Â§™Êúó",
    181:"Á´πÊùëÈÅî‰πüËâØ",182:"È¥®ÂÆÆÁ••Ë°åËâØ",183:"ÊùæÊú¨ÂâõÂè≤ËâØ",184:"Â∞èÁâßÂ§™ËâØ",185:"ÂêâÊùëÊô∫Ê¥ãËâØ",186:"‰∏ãÂéüÁêÜÈöÜ",187:"Âª£ÁÄ¨Ëà™ËâØ",188:"Èï∑Ë∞∑ÈÉ®ÈßøÂº•ËâØ",189:"‰∏≠Ë∂äÁêâ‰∏ñËâØ",190:"Áî∞‰∏≠Â≠¶Áúü",
    191:"Èï∑Áî∞ÈÄ≤‰ªÅËâØ",192:"‰ΩêÂéüÁßÄÊ≥∞ËâØ",193:"Â§ßÊüø‰∏ÄÁúüÈöÜ",194:"È´òÈáéË™†ÊØÖËâØ",195:"Â±±Áî∞ÈõÑÂ§ßËâØ",196:"Ê±†Ë∞∑Âå†ÁøîËâØ",197:"Â∞èÁâßÂ§™ÈöÜ",198:"Áü≥Â∑ùÊÖéÂ∞ÜËâØ",199:"ÂêâÊùëË™†‰πãÂä©ËâØ",200:"Â±±Êú¨ËÅ°ÂìâËâØ",
}
_JOCKEY_NAME_TO_RANK: Dict[str, int] = { _normalize_name(v): k for k, v in JOCKEY_RANK_TABLE_RAW.items() }

def _best_match_rank(name_norm: str) -> Optional[int]:
    """
    Áõ¥Êé•‰∏ÄËá¥„Åå„Å™„ÅÑÂ†¥Âêà„ÅÆ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØÔºö
      1) ÂâçÊñπ‰∏ÄËá¥
      2) ÈÄÜÂâçÊñπ‰∏ÄËá¥Ôºà„ÉÜ„Éº„Éñ„É´ÂÅ¥„ÅåÁü≠„ÅÑÂ†¥ÂêàÔºâ
      ÂÄôË£ú„ÅåË§áÊï∞„Å™„Çâ ‚ÄúÊñáÂ≠óÂàóÂ∑Æ„ÅåÂ∞è‚Äù ‚Üí ‚Äú„É©„É≥„ÇØ‰∏ä‰ΩçÔºàÊï∞ÂÄ§„ÅåÂ∞è„Åï„ÅÑÔºâ‚Äù „ÇíÂÑ™ÂÖà
    """
    candidates = []
    for n2, rank in _JOCKEY_NAME_TO_RANK.items():
        if n2.startswith(name_norm) or name_norm.startswith(n2):
            diff = abs(len(n2) - len(name_norm))
            candidates.append((diff, rank))
    if not candidates:
        return None
    candidates.sort(key=lambda x: (x[0], x[1]))
    return candidates[0][1]

def jockey_rank_letter_by_name(name: Optional[str]) -> str:
    """Ë°®Á§∫„É©„É≥„ÇØ: A=1„Äú70 / B=71„Äú200 / C=„Åù„ÅÆ‰ªñ / ‚Äî=ÂêçÂâç„Å™„Åó"""
    if not name:
        return "‚Äî"
    base_raw = _clean_jockey_name(name)
    base = _normalize_name(base_raw)
    rank = _JOCKEY_NAME_TO_RANK.get(base)
    if rank is None and base:
        rank = _best_match_rank(base)
    if rank is None:
        _log_rank_miss(base_raw, base)
        return "C"
    return "A" if 1 <= rank <= 70 else ("B" if 71 <= rank <= 200 else "C")

# ========= ÂÖ±ÈÄö =========
def now_jst() -> datetime: return datetime.now(JST)

def within_operating_hours() -> bool:
    if FORCE_RUN: return True
    return START_HOUR <= now_jst().hour < END_HOUR

def fetch(url: str) -> str:
    last_err=None
    for i in range(1, RETRY+1):
        try:
            r=SESSION.get(url, timeout=TIMEOUT); r.raise_for_status()
            r.encoding="utf-8"; return r.text
        except Exception as e:
            last_err=e; wait=random.uniform(*SLEEP_BETWEEN)
            logging.warning(f"[WARN] fetchÂ§±Êïó({i}/{RETRY}) {e} -> {wait:.1f}sÂæÖÊ©ü: {url}")
            time.sleep(wait)
    raise last_err

# ========= Google Sheets =========
def _sheet_service():
    if not GOOGLE_CREDENTIALS_JSON or not GOOGLE_SHEET_ID:
        raise RuntimeError("Google Sheets „ÅÆÁí∞Â¢ÉÂ§âÊï∞‰∏çË∂≥")
    info=json.loads(GOOGLE_CREDENTIALS_JSON)
    creds=Credentials.from_service_account_info(info, scopes=["https://www.googleapis.com/auth/spreadsheets"])
    return build("sheets","v4",credentials=creds, cache_discovery=False)

def _resolve_sheet_title(svc, tab_or_gid: str) -> str:
    meta=svc.spreadsheets().get(spreadsheetId=GOOGLE_SHEET_ID).execute()
    sheets=meta.get("sheets", [])
    if tab_or_gid.isdigit() and len(tab_or_gid)>3:
        gid=int(tab_or_gid)
        for s in sheets:
            if s["properties"]["sheetId"]==gid:
                return s["properties"]["title"]
        raise RuntimeError(f"ÊåáÂÆögid„ÅÆ„Ç∑„Éº„Éà„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì: {gid}")
    for s in sheets:
        if s["properties"]["title"]==tab_or_gid:
            return tab_or_gid
    body={"requests":[{"addSheet":{"properties":{"title":tab_or_gid}}}]}
    svc.spreadsheets().batchUpdate(spreadsheetId=GOOGLE_SHEET_ID, body=body).execute()
    return tab_or_gid

def _sheet_get_range_values(svc, title: str, a1: str) -> List[List[str]]:
    res=svc.spreadsheets().values().get(spreadsheetId=GOOGLE_SHEET_ID, range=f"'{title}'!{a1}").execute()
    return res.get("values", [])

def _sheet_update_range_values(svc, title: str, a1: str, values: List[List[str]]):
    svc.spreadsheets().values().update(
        spreadsheetId=GOOGLE_SHEET_ID, range=f"'{title}'!{a1}",
        valueInputOption="RAW", body={"values": values}).execute()

def sheet_load_notified() -> Dict[str, float]:
    svc=_sheet_service(); title=_resolve_sheet_title(svc, GOOGLE_SHEET_TAB)
    values=_sheet_get_range_values(svc, title, "A:C")
    start = 1 if values and values[0] and str(values[0][0]).upper() in ("KEY","RACEID","RID","ID") else 0
    d={}
    for row in values[start:]:
        if not row or len(row)<2: continue
        key=str(row[0]).strip()
        try: d[key]=float(row[1])
        except: pass
    return d

def sheet_upsert_notified(key: str, ts: float, note: str = "") -> None:
    svc=_sheet_service(); title=_resolve_sheet_title(svc, GOOGLE_SHEET_TAB)
    values=_sheet_get_range_values(svc, title, "A:C")
    header=["KEY","TS_EPOCH","NOTE"]
    if not values:
        _sheet_update_range_values(svc, title, "A:C", [header, [key, ts, note]]); return
    start_row=1 if values and values[0] and values[0][0] in header else 0
    found=None
    for i,row in enumerate(values[start_row:], start=start_row):
        if row and str(row[0]).strip()==key: found=i; break
    if found is None: values.append([key, ts, note])
    else: values[found]=[key, ts, note]
    _sheet_update_range_values(svc, title, "A:C", values)

# ========= ÈÄÅ‰ø°ÂÖà„É¶„Éº„Ç∂„Éº =========
def load_user_ids_from_simple_col() -> List[str]:
    svc=_sheet_service(); title=USERS_SHEET_NAME; col=USERS_USERID_COL.upper()
    values=_sheet_get_range_values(svc, title, f"{col}:{col}")
    user_ids=[]
    for i,row in enumerate(values):
        v=(row[0].strip() if row and row[0] is not None else "")
        if not v: continue
        low=v.replace(" ","").lower()
        if i==0 and ("userid" in low or "user id" in low or "line" in low): continue
        if not v.startswith("U"): continue
        if v not in user_ids: user_ids.append(v)
    logging.info("[INFO] users„Ç∑„Éº„ÉàË™≠Ëæº: %d‰ª∂ from tab=%s", len(user_ids), title)
    return user_ids

# ========= HTML„É¶„Éº„ÉÜ„Ç£„É™„ÉÜ„Ç£ =========
def _extract_raceids_from_soup(soup: BeautifulSoup) -> List[str]:
    rids=[]
    for a in soup.find_all("a", href=True):
        m=RACEID_RE.search(a["href"])
        if m:
            rid=m.group(1)
            if not PLACEHOLDER.search(rid): rids.append(rid)
    return sorted(set(rids))

def _rid_date_parts(rid: str) -> Tuple[int,int,int]:
    return int(rid[0:4]), int(rid[4:6]), int(rid[6:8])

def _norm_hhmm_from_text(text: str) -> Optional[Tuple[int,int,str]]:
    if not text: return None
    s=str(text)
    for pat, tag in zip(TIME_PATS, ("half","full","kanji")):
        m=pat.search(s)
        if m:
            hh=int(m.group(1)); mm=int(m.group(2))
            if 0<=hh<=23 and 0<=mm<=59: return hh,mm,tag
    return None

def _make_dt_from_hhmm(rid: str, hh: int, mm: int) -> Optional[datetime]:
    try:
        y,mon,d=_rid_date_parts(rid)
        return datetime(y,mon,d,hh,mm,tzinfo=JST)
    except: return None

def _find_time_nearby(el: Tag) -> Tuple[Optional[str], str]:
    t=el.find("time")
    if t:
        for attr in ("datetime","data-time","title","aria-label"):
            v=t.get(attr)
            if v:
                got=_norm_hhmm_from_text(v)
                if got: hh,mm,why=got; return f"{hh:02d}:{mm:02d}", f"time@{attr}/{why}"
        got=_norm_hhmm_from_text(t.get_text(" ", strip=True))
        if got: hh,mm,why=got; return f"{hh:02d}:{mm:02d}", f"time@text/{why}"
    for node in el.find_all(True, recursive=True):
        for attr in ("data-starttime","data-start-time","data-time","title","aria-label"):
            v=node.get(attr)
            if not v: continue
            got=_norm_hhmm_from_text(v)
            if got: hh,mm,why=got; return f"{hh:02d}:{mm:02d}", f"data:{attr}/{why}"
    for sel in [".startTime",".cellStartTime",".raceTime",".time",".start-time"]:
        node=el.select_one(sel)
        if node:
            got=_norm_hhmm_from_text(node.get_text(" ", strip=True))
            if got: hh,mm,why=got; return f"{hh:02d}:{mm:02d}", f"sel:{sel}/{why}"
    got=_norm_hhmm_from_text(el.get_text(" ", strip=True))
    if got: hh,mm,why=got; return f"{hh:02d}:{mm:02d}", f"row:text/{why}"
    return None, "-"

# ========= Áô∫Ëµ∞ÊôÇÂàªÔºà‰∏ÄË¶ß„Éö„Éº„Ç∏ÔºâËß£Êûê =========
def parse_post_times_from_table_like(root: Tag) -> Dict[str, datetime]:
    post_map={}
    # „ÉÜ„Éº„Éñ„É´
    for table in root.find_all("table"):
        thead=table.find("thead")
        if thead:
            head_text="".join(thead.stripped_strings)
            if not any(k in head_text for k in ("Áô∫Ëµ∞","Áô∫Ëµ∞ÊôÇÂàª","„É¨„Éº„Çπ")): continue
        body=table.find("tbody") or table
        for tr in body.find_all("tr"):
            rid=None; link=tr.find("a", href=True)
            if link:
                m=RACEID_RE.search(link["href"]); 
                if m: rid=m.group(1)
            if not rid or PLACEHOLDER.search(rid): continue
            hhmm,_=_find_time_nearby(tr)
            if not hhmm: continue
            hh,mm=map(int, hhmm.split(":"))
            dt=_make_dt_from_hhmm(rid, hh, mm)
            if dt: post_map[rid]=dt
    # „Ç´„Éº„ÉâÂûã
    for a in root.find_all("a", href=True):
        m=RACEID_RE.search(a["href"])
        if not m: continue
        rid=m.group(1)
        if PLACEHOLDER.search(rid) or rid in post_map: continue
        host=None; depth=0
        for parent in a.parents:
            if isinstance(parent, Tag) and parent.name in ("tr","li","div","section","article"):
                host=parent; break
            depth+=1
            if depth>=6: break
        host=host or a
        hhmm,_=_find_time_nearby(host)
        if not hhmm:
            sib_text=" ".join([x.get_text(" ", strip=True) for x in a.find_all_next(limit=4) if isinstance(x, Tag)])
            got=_norm_hhmm_from_text(sib_text)
            if got:
                hh,mm,_=got; hhmm=f"{hh:02d}:{mm:02d}"
        if not hhmm: continue
        hh,mm=map(int, hhmm.split(":"))
        dt=_make_dt_from_hhmm(rid, hh, mm)
        if dt: post_map[rid]=dt
    return post_map

def collect_post_time_map(ymd: str, ymd_next: str) -> Dict[str, datetime]:
    post_map={}
    def _merge_from(url: str):
        try:
            soup=BeautifulSoup(fetch(url), "lxml")
            post_map.update(parse_post_times_from_table_like(soup))
        except Exception as e:
            logging.warning(f"[WARN] Áô∫Ëµ∞‰∏ÄË¶ßË™≠„ÅøËæº„ÅøÂ§±Êïó: {e} ({url})")
    _merge_from(f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{ymd}0000000000")
    _merge_from(f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{ymd_next}0000000000")
    logging.info(f"[INFO] Áô∫Ëµ∞ÊôÇÂàªÂèñÂæó: {len(post_map)}‰ª∂")
    return post_map

# ========= Á∑†ÂàáÊôÇÂàªÔºàÊúÄÂÑ™ÂÖà„ÅßÊäΩÂá∫Ôºâ =========
def _extract_cutoff_hhmm_from_soup(soup: BeautifulSoup) -> Optional[str]:
    # „Çª„É¨„ÇØ„ÇøÂÑ™ÂÖà
    for sel in ["time[data-type='cutoff']", ".cutoff time", ".deadline time", ".time.-deadline"]:
        t=soup.select_one(sel)
        if t:
            got=_norm_hhmm_from_text(t.get_text(" ", strip=True) or t.get("datetime",""))
            if got:
                hh,mm,_=got; return f"{hh:02d}:{mm:02d}"
    # „É©„Éô„É´ËøëÂÇçÔºà‚ÄúÁ∑†Âàá‚Äù„ÉØ„Éº„Éâ„ÅÆÂë®Ëæ∫Ôºâ
    for node in soup.find_all(string=CUTOFF_LABEL_PAT):
        container=getattr(node, "parent", None) or soup
        host=container
        for p in container.parents:
            if isinstance(p, Tag) and p.name in ("div","section","article","li"): host=p; break
        text=" ".join(host.get_text(" ", strip=True).split())
        got=_norm_hhmm_from_text(text)
        if got:
            hh,mm,_=got; return f"{hh:02d}:{mm:02d}"
    # ÂÖ®Êñá„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
    txt=" ".join(soup.stripped_strings)
    if CUTOFF_LABEL_PAT.search(txt):
        got=_norm_hhmm_from_text(txt)
        if got:
            hh,mm,_=got; return f"{hh:02d}:{mm:02d}"
    return None

def resolve_cutoff_dt(rid: str) -> Optional[Tuple[datetime, str]]:
    # ÂçòË§á„Éö„Éº„Ç∏ÂÑ™ÂÖà
    try:
        soup=BeautifulSoup(fetch(f"https://keiba.rakuten.co.jp/odds/tanfuku/RACEID/{rid}"), "lxml")
        hhmm=_extract_cutoff_hhmm_from_soup(soup)
        if hhmm:
            hh,mm=map(int, hhmm.split(":"))
            dt=_make_dt_from_hhmm(rid, hh, mm)
            if dt: return dt, "tanfuku"
    except Exception as e:
        logging.warning("[WARN] Á∑†ÂàáÊäΩÂá∫(tanfuku)Â§±Êïó rid=%s: %s", rid, e)
    # ‰∏ÄË¶ß„Éö„Éº„Ç∏„Åß„ÇÇË©¶„Åô
    try:
        soup=BeautifulSoup(fetch(f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{rid}"), "lxml")
        hhmm=_extract_cutoff_hhmm_from_soup(soup)
        if hhmm:
            hh,mm=map(int, hhmm.split(":"))
            dt=_make_dt_from_hhmm(rid, hh, mm)
            if dt: return dt, "list"
    except Exception as e:
        logging.warning("[WARN] Á∑†ÂàáÊäΩÂá∫(list)Â§±Êïó rid=%s: %s", rid, e)
    return None

# ========= „Ç™„ÉÉ„Ç∫Ëß£ÊûêÔºàÂçòË§á„Éö„Éº„Ç∏Ôºâ =========
def _clean(s: str) -> str: return re.sub(r"\s+","", s or "")
def _as_float(text: str) -> Optional[float]:
    if not text: return None
    t=text.replace(",","").strip()
    if "%" in t or "-" in t or "ÔΩû" in t or "~" in t: return None
    m=re.search(r"\d+(?:\.\d+)?", t); return float(m.group(0)) if m else None
def _as_int(text: str) -> Optional[int]:
    if not text: return None
    m=re.search(r"\d+", text); return int(m.group(0)) if m else None

def _find_popular_odds_table(soup: BeautifulSoup) -> Tuple[Optional[BeautifulSoup], Dict[str,int]]:
    for table in soup.find_all("table"):
        thead=table.find("thead")
        if not thead: continue
        headers=[_clean(th.get_text()) for th in thead.find_all(["th","td"])]
        if not headers: continue
        pop_idx=win_idx=num_idx=jockey_idx=None
        for i,h in enumerate(headers):
            if h in ("‰∫∫Ê∞ó","È†Ü‰Ωç") or ("‰∫∫Ê∞ó" in h and "È†Ü" not in h): pop_idx=i; break
        win_c=[]
        for i,h in enumerate(headers):
            if ("Ë§á" in h) or ("Áéá" in h) or ("%" in h): continue
            if h=="ÂçòÂãù": win_c.append((0,i))
            elif "ÂçòÂãù" in h: win_c.append((1,i))
            elif "„Ç™„ÉÉ„Ç∫" in h: win_c.append((2,i))
        win_idx=sorted(win_c,key=lambda x:x[0])[0][1] if win_c else None
        for i,h in enumerate(headers):
            if "È¶¨Áï™" in h: num_idx=i; break
        if num_idx is None:
            for i,h in enumerate(headers):
                if ("È¶¨" in h) and ("È¶¨Âêç" not in h) and (i!=pop_idx): num_idx=i; break
        for i,h in enumerate(headers):
            if any(k in h for k in ("È®éÊâã","È®éÊâãÂêç")): jockey_idx=i; break
        if pop_idx is None or win_idx is None: continue
        body=table.find("tbody") or table
        rows=body.find_all("tr")
        seq_ok,last=0,0
        for tr in rows[:6]:
            tds=tr.find_all(["td","th"])
            if len(tds)<=max(pop_idx,win_idx): continue
            s=tds[pop_idx].get_text(strip=True)
            if not s.isdigit(): break
            v=int(s)
            if v<=last: break
            last=v; seq_ok+=1
        if seq_ok>=2:
            return table, {"pop":pop_idx,"win":win_idx,"num":num_idx if num_idx is not None else -1,"jockey":jockey_idx if jockey_idx is not None else -1}
    return None, {}

def parse_odds_table(soup: BeautifulSoup) -> Tuple[List[Dict[str,float]], Optional[str], Optional[str]]:
    venue_race=(soup.find("h1").get_text(strip=True) if soup.find("h1") else None)
    nowtime=soup.select_one(".withUpdate .nowTime") or soup.select_one(".nowTime")
    now_label=nowtime.get_text(strip=True) if nowtime else None
    table, idx=_find_popular_odds_table(soup)
    if not table: return [], venue_race, now_label
    pop_idx=idx["pop"]; win_idx=idx["win"]; num_idx=idx.get("num",-1); jockey_idx=idx.get("jockey",-1)
    horses=[]; body=table.find("tbody") or table
    for tr in body.find_all("tr"):
        tds=tr.find_all(["td","th"])
        if len(tds)<=max(pop_idx,win_idx): continue
        pop_txt=tds[pop_idx].get_text(strip=True)
        if not pop_txt.isdigit(): continue
        pop=int(pop_txt)
        if not (1<=pop<=30): continue
        odds=_as_float(tds[win_idx].get_text(" ", strip=True))
        if odds is None: continue
        num=None
        if 0<=num_idx<len(tds): num=_as_int(tds[num_idx].get_text(" ", strip=True))
        jockey=None
        if 0<=jockey_idx<len(tds):
            jt=tds[jockey_idx].get_text(" ", strip=True)
            jraw=re.split(r"[Ôºà( ]", jt)[0].strip() if jt else None
            jclean=_clean_jockey_name(jraw) if jraw else None
            jockey=jclean if jclean else None
        rec={"pop":pop,"odds":float(odds)}
        if num is not None: rec["num"]=num
        if jockey: rec["jockey"]=jockey
        horses.append(rec)
    uniq={}
    for h in sorted(horses, key=lambda x:x["pop"]): uniq[h["pop"]]=h
    horses=[uniq[k] for k in sorted(uniq.keys())]
    return horses, venue_race, now_label

# === Âá∫È¶¨Ë°®„Åã„Çâ„ÅÆÈ®éÊâãË£úÂÆå ===
def fetch_jockey_map_from_card(race_id: str) -> Dict[int, str]:
    urls=[f"https://keiba.rakuten.co.jp/race_card/RACEID/{race_id}",
          f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{race_id}"]
    result={}
    for url in urls:
        try: soup=BeautifulSoup(fetch(url),"lxml")
        except Exception: continue
        for table in soup.find_all("table"):
            thead=table.find("thead")
            if not thead: continue
            headers=[_clean(th.get_text()) for th in thead.find_all(["th","td"])]
            if not headers: continue
            num_idx=next((i for i,h in enumerate(headers) if "È¶¨Áï™" in h), -1)
            jockey_idx=next((i for i,h in enumerate(headers) if any(k in h for k in ("È®éÊâã","È®éÊâãÂêç"))), -1)
            if num_idx<0 or jockey_idx<0: continue
            body=table.find("tbody") or table
            for tr in body.find_all("tr"):
                tds=tr.find_all(["td","th"])
                if len(tds)<=max(num_idx, jockey_idx): continue
                num=_as_int(tds[num_idx].get_text(" ", strip=True))
                jtx=tds[jockey_idx].get_text(" ", strip=True)
                if num is None or not jtx: continue
                name=_clean_jockey_name(re.split(r"[Ôºà(]", jtx)[0])
                if name: result[num]=name
            if result: return result
    return result

def _enrich_horses_with_jockeys(horses: List[Dict[str,float]], race_id: str) -> None:
    need=any((h.get("jockey") is None) and isinstance(h.get("num"), int) for h in horses)
    if not need: return
    num2jockey=fetch_jockey_map_from_card(race_id)
    if not num2jockey: return
    for h in horses:
        if not h.get("jockey") and isinstance(h.get("num"), int):
            name=num2jockey.get(h["num"])
            if name: h["jockey"]=name

def check_tanfuku_page(race_id: str) -> Optional[Dict]:
    url=f"https://keiba.rakuten.co.jp/odds/tanfuku/RACEID/{race_id}"
    html=fetch(url); soup=BeautifulSoup(html,"lxml")
    horses, venue_race, now_label = parse_odds_table(soup)
    if not horses: return None
    if not venue_race: venue_race="Âú∞ÊñπÁ´∂È¶¨"
    _enrich_horses_with_jockeys(horses, race_id)
    return {"race_id": race_id, "url": url, "horses": horses, "venue_race": venue_race, "now": now_label or ""}

# ========= Áô∫Ëµ∞ÊôÇÂàª„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ =========
def fallback_post_time_for_rid(rid: str) -> Optional[Tuple[datetime, str, str]]:
    def _from_list_page():
        url=f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{rid}"
        soup=BeautifulSoup(fetch(url),"lxml")
        a=soup.find("a", href=re.compile(rf"/RACEID/{rid}"))
        if not a: return None
        host=None
        for parent in a.parents:
            if isinstance(parent, Tag) and parent.name in ("tr","li","div","section","article"):
                host=parent; break
        host=host or a
        hhmm,reason=_find_time_nearby(host)
        if not hhmm:
            sibs=[n for n in host.find_all_next(limit=6) if isinstance(n, Tag)]
            text=" ".join([n.get_text(" ", strip=True) for n in sibs])
            if POST_LABEL_PAT.search(text):
                got=_norm_hhmm_from_text(text)
                if got:
                    hh,mm,why=got; hhmm,reason=f"{hh:02d}:{mm:02d}", f"sibling:label-first/{why}"
            else:
                if not IGNORE_NEAR_PAT.search(text):
                    got=_norm_hhmm_from_text(text)
                    if got:
                        hh,mm,why=got; hhmm,reason=f"{hh:02d}:{mm:02d}", f"sibling:text/{why}"
        if not hhmm: return None
        hh,mm=map(int, hhmm.split(":")); dt=_make_dt_from_hhmm(rid, hh, mm)
        return (dt, f"list-anchor/{reason}", url) if dt else None

    def _from_tanfuku_page():
        url=f"https://keiba.rakuten.co.jp/odds/tanfuku/RACEID/{rid}"
        soup=BeautifulSoup(fetch(url),"lxml")
        for key in ("Áô∫Ëµ∞","Áô∫Ëµ∞ÊôÇÂàª","Áô∫Ëµ∞‰∫àÂÆö","Áô∫ÈÄÅ","Âá∫Ëµ∞"):
            for node in soup.find_all(string=re.compile(key)):
                el=getattr(node,"parent",None) or soup
                container=el
                for parent in el.parents:
                    if isinstance(parent, Tag) and parent.name in ("div","section","article","li"):
                        container=parent; break
                near=" ".join((container.get_text(" ", strip=True) or "").split())
                if IGNORE_NEAR_PAT.search(near) and not POST_LABEL_PAT.search(near):
                    continue
                got=_norm_hhmm_from_text(near)
                if got:
                    hh,mm,why=got; dt=_make_dt_from_hhmm(rid, hh, mm)
                    if dt: return dt, f"tanfuku-label/{key}/{why}", url
        return None

    try:
        got=_from_list_page()
        if got: return got
    except Exception as e:
        logging.warning("[WARN] fallback(list)Â§±Êïó rid=%s: %s", rid, e)
    try:
        got=_from_tanfuku_page()
        if got: return got
    except Exception as e:
        logging.warning("[WARN] fallback(tanfuku)Â§±Êïó rid=%s: %s", rid, e)
    return None

# ========= RACEID ÂàóÊåô =========
def list_raceids_today_ticket(ymd: str) -> List[str]:
    url=f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{ymd}0000000000"
    soup=BeautifulSoup(fetch(url),"lxml")
    ids=_extract_raceids_from_soup(soup)
    logging.info(f"[INFO] Rakuten#1 Êú¨Êó•„ÅÆÁô∫Â£≤ÊÉÖÂ†±: {len(ids)}‰ª∂")
    return ids

def list_raceids_from_card_lists(ymd: str, ymd_next: str) -> List[str]:
    urls=[f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{ymd}0000000000",
          f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{ymd_next}0000000000"]
    rids=[]
    for u in urls:
        try:
            soup=BeautifulSoup(fetch(u),"lxml")
            rids.extend(_extract_raceids_from_soup(soup))
        except Exception as e:
            logging.warning(f"[WARN] Âá∫È¶¨Ë°®‰∏ÄË¶ß„Çπ„Ç≠„É£„É≥Â§±Êïó: {e} ({u})")
    rids=sorted(set(rids))
    logging.info(f"[INFO] Rakuten#2 Âá∫È¶¨Ë°®‰∏ÄË¶ß: {len(rids)}‰ª∂")
    return rids

# ========= Á™ìÂà§ÂÆöÔºà„Çø„Éº„Ç≤„ÉÉ„ÉàÊôÇÂàªÂü∫Ê∫ñÔºã„Ç∞„É¨„Éº„ÇπÔºâ =========
def is_within_window(target_dt: datetime, now: datetime, before_min:int=WINDOW_BEFORE_MIN,
                     after_min:int=WINDOW_AFTER_MIN, grace_sec:int=GRACE_SECONDS) -> bool:
    win_start = target_dt - timedelta(minutes=before_min, seconds=grace_sec)
    win_end   = target_dt + timedelta(minutes=after_min,  seconds=grace_sec)
    return win_start <= now <= win_end

# ========= LINE =========
def push_line_text(user_id: str, token: str, text: str, timeout=8, retries=1):
    headers={"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    payload={"to": user_id, "messages":[{"type":"text","text": text}]}
    for attempt in range(retries+1):
        try:
            resp=requests.post(LINE_PUSH_URL, headers=headers, json=payload, timeout=timeout)
            if resp.status_code==200: return True, 200, resp.text
            if resp.status_code==429 and attempt<retries:
                wait=int(resp.headers.get("Retry-After","1")); time.sleep(max(wait,1)); continue
            return False, resp.status_code, resp.text
        except requests.RequestException as e:
            if attempt<retries: time.sleep(2); continue
            return False, None, str(e)

def notify_strategy_hit_to_many(message_text: str, targets: List[str]):
    if not NOTIFY_ENABLED: logging.info("[INFO] NOTIFY_ENABLED=0"); return False, None
    if DRY_RUN: logging.info("[DRY_RUN] ÈÄöÁü•:\n%s", message_text); return False, None
    if not LINE_ACCESS_TOKEN: logging.error("[ERROR] LINE_ACCESS_TOKEN ‰∏çË∂≥"); return False, None
    if not targets: logging.error("[ERROR] ÈÄÅ‰ø°ÂÖà„É¶„Éº„Ç∂„Éº„Å™„Åó"); return False, None
    all_ok=True; last=None
    for uid in targets:
        ok, status, _ = push_line_text(uid, LINE_ACCESS_TOKEN, message_text)
        last=status
        if not ok: all_ok=False
        time.sleep(0.2)
    return all_ok, last

# ========= ÈÄöÁü•„ÉÜ„Ç≠„Çπ„ÉàÂÖ±ÈÄö =========
_CIRCLED="‚ë†‚ë°‚ë¢‚ë£‚ë§‚ë•‚ë¶‚ëß‚ë®"
def _circled(n:int)->str: return _CIRCLED[n-1] if 1<=n<=9 else f"{n}."
def _extract_hhmm_label(s:str)->Optional[str]:
    got=_norm_hhmm_from_text(s)
    if not got: return None
    hh,mm,_=got; return f"{hh:02d}:{mm:02d}"
def _infer_pattern_no(strategy_text: str) -> int:
    if not strategy_text: return 0
    m=re.match(r"\s*([‚ë†-‚ë®])", strategy_text)
    if m: return _CIRCLED.index(m.group(1))+1
    m=re.match(r"\s*(\d+)", strategy_text)
    if m:
        try: return int(m.group(1))
        except: return 0
    return 0
def _strip_pattern_prefix(strategy_text: str) -> str:
    s=re.sub(r"^\s*[‚ë†-‚ë®]\s*", "", strategy_text or "")
    s=re.sub(r"^\s*\d+\s*", "", s); return s.strip()
def _split_venue_race(venue_race: str) -> Tuple[str,str]:
    if not venue_race: return "Âú∞ÊñπÁ´∂È¶¨",""
    m=re.search(r"^\s*([^\s\d]+)\s*(\d{1,2}R)\b", venue_race)
    if m:
        venue=m.group(1); race=m.group(2)
        venue_disp = f"{venue}Á´∂È¶¨Â†¥" if "Á´∂È¶¨" not in venue else venue
        return venue_disp, race
    return venue_race, ""

# ==== „ÄåÂçòÂãù‰∫∫Ê∞óÔºçÈ®éÊâã„É©„É≥„ÇØ„ÄçË°®Á§∫Áî®„ÉÑ„Éº„É´Áæ§ ====
def _map_pop_info(horses: List[Dict[str,float]]) -> Dict[int, Dict[str, Optional[float]]]:
    m={}
    for h in horses:
        try:
            p=int(h.get("pop"))
            num=h.get("num") if isinstance(h.get("num"), int) else None
            o=float(h.get("odds")) if h.get("odds") is not None else None
            j=h.get("jockey") or None
            m[p]={"umaban":num,"odds":o,"jockey":j}
        except: pass
    return m

def _map_umaban_to_pop(horses: List[Dict[str,float]]) -> Dict[int, int]:
    out={}
    for h in horses:
        try:
            p=int(h.get("pop"))
            n=int(h.get("num")) if h.get("num") is not None else None
            if n is not None: out[n]=p
        except: pass
    return out

def _pop_rank_label(p: int, horses: List[Dict[str,float]]) -> str:
    info=_map_pop_info(horses).get(p, {})
    jk=info.get("jockey"); r=jockey_rank_letter_by_name(jk) if jk else "‚Äî"
    return f"{p}Ôºç{r}"

def _format_bets_pop_rank(bets: List[str], horses: List[Dict[str,float]]) -> List[str]:
    pop2=_map_pop_info(horses); uma2pop=_map_umaban_to_pop(horses)
    pops_set=set(pop2.keys()); umaban_set=set(uma2pop.keys())
    out=[]
    for b in bets:
        nums=[int(x) for x in re.findall(r"\d+", str(b))]
        if not nums: out.append(b); continue
        if all(n in pops_set for n in nums):
            seq=[_pop_rank_label(n, horses) for n in nums]; out.append("-".join(seq))
        elif all(n in umaban_set for n in nums):
            seq=[]; ok=True
            for n in nums:
                p=uma2pop.get(n); 
                if p is None: ok=False; break
                seq.append(_pop_rank_label(p, horses))
            out.append("-".join(seq) if ok else b)
        else:
            out.append(b)
    return out

# ÈÄöÁü•Êú¨ÊñáÔºà‚ë†‚ë°‚ë£ ÂÖ±ÈÄöÔºâ
def build_line_notification(pattern_no:int, venue:str, race_no:str, time_label:str, time_hm:str,
                            condition_text:str, raw_bets:List[str], odds_timestamp_hm:Optional[str],
                            odds_url:str, horses:List[Dict[str,float]]) -> str:
    title=f"„ÄêÊà¶Áï•{pattern_no if pattern_no>0 else ''}Ë©≤ÂΩì„É¨„Éº„ÇπÁô∫Ë¶ãüí°„Äë".replace("Êà¶Áï•Ë©≤ÂΩì","Êà¶Áï•Ë©≤ÂΩì")
    lines=[title, f"‚ñ†„É¨„Éº„ÇπÔºö{venue} {race_no}Ôºà{time_label} {time_hm}Ôºâ".strip()]
    if condition_text: lines.append(f"‚ñ†Êù°‰ª∂Ôºö{condition_text}")
    lines+=["", "‚ñ†Ë≤∑„ÅÑÁõÆÔºàÂçòÂãù‰∫∫Ê∞óÔºçÈ®éÊâã„É©„É≥„ÇØÔºâÔºö"]
    pretty=_format_bets_pop_rank(raw_bets, horses)
    for i,bet in enumerate(pretty,1): lines.append(f"{_circled(i)} {bet}")
    if odds_timestamp_hm: lines+=["", f"üìÖ „Ç™„ÉÉ„Ç∫ÊôÇÁÇπ: {odds_timestamp_hm}"]
    lines+=["üîó „Ç™„ÉÉ„Ç∫Ë©≥Á¥∞:", odds_url, ""]
    lines+=[
        "‚Äª„Ç™„ÉÉ„Ç∫„ÅØÁ∑†ÂàáÁõ¥Ââç„Åæ„ÅßÂ§âÂãï„Åó„Åæ„Åô„ÄÇ",
        "‚ÄªÈ®éÊâã„É©„É≥„ÇØ„ÅØ2024Âπ¥„ÅÆÂú∞ÊñπÁ´∂È¶¨„É™„Éº„Éá„Ç£„É≥„Ç∞„Ç∏„Éß„ÉÉ„Ç≠„Éº„É©„É≥„Ç≠„É≥„Ç∞„Å´Âü∫„Å•„Åç„ÄÅA=1„Äú70‰Ωç / B=71„Äú200‰Ωç / C=„Åù„ÅÆ‰ªñ„ÄÇ",
        "‚ÄªÈ¶¨Âà∏Ë≥ºÂÖ•„ÅØ‰ΩôË£ïË≥áÈáë„Åß„ÄÇÁöÑ‰∏≠„ÅØ‰øùË®º„Åï„Çå„Åæ„Åõ„Çì„ÄÇ"
    ]
    return "\n".join(lines)

# ‚ë¢Â∞ÇÁî®
def build_line_notification_strategy3(strategy:Dict, venue:str, race_no:str, time_label:str, time_hm:str,
                                      odds_timestamp_hm:Optional[str], odds_url:str,
                                      horses:List[Dict[str,float]]) -> str:
    pop2=_map_pop_info(horses)
    axis=strategy.get("axis") or {}
    axis_pop=axis.get("pop") or 1
    axis_rank=jockey_rank_letter_by_name((pop2.get(axis_pop) or {}).get("jockey"))
    axis_label=f"{axis_pop}Ôºç{axis_rank}"

    cands=strategy.get("candidates")
    if not cands:
        cands=[]
        for h in sorted(horses, key=lambda x:int(x.get("pop",999))):
            try:
                p=int(h.get("pop")); o=float(h.get("odds"))
                if p==1: continue
                if 10.0<=o<=20.0:
                    cands.append({"pop":p,"jockey":h.get("jockey")})
                    if len(cands)>=4: break
            except: pass

    def _cand_label(c):
        r=jockey_rank_letter_by_name(c.get("jockey"))
        return f"{c.get('pop','-')}Ôºç{r}"
    cand_labels=[_cand_label(c) for c in sorted(cands, key=lambda x:x.get("pop",999))]

    tickets=strategy.get("tickets") or []
    pretty=_format_bets_pop_rank(tickets, horses)

    title="„ÄêÊà¶Áï•‚ë¢Ë©≤ÂΩì„É¨„Éº„ÇπÁô∫Ë¶ãüí°„Äë"
    cond_line="1Áï™‰∫∫Ê∞ó ‚â§2.0„ÄÅ2Áï™‰∫∫Ê∞ó ‚â•10.0„ÄÅÁõ∏ÊâãÔºùÂçòÂãù10„Äú20ÂÄçÔºàÊúÄÂ§ß4È†≠Ôºâ"
    n=len(cand_labels); pts=n*(n-1) if n>=2 else 0

    lines=[title,
           f"‚ñ†„É¨„Éº„ÇπÔºö{venue} {race_no}Ôºà{time_label} {time_hm}Ôºâ",
           f"‚ñ†Êù°‰ª∂Ôºö{cond_line}",
           f"‚ñ†Ëª∏ÔºàÂçòÂãù‰∫∫Ê∞óÔºçÈ®éÊâã„É©„É≥„ÇØÔºâÔºö{axis_label}",
           f"‚ñ†Áõ∏ÊâãÂÄôË£úÔºà‰∫∫Ê∞óÔºç„É©„É≥„ÇØÔºâÔºö{', '.join(cand_labels) if cand_labels else '‚Äî'}",
           f"‚ñ†Ë≤∑„ÅÑÁõÆÔºà3ÈÄ£Âçò„Éª1ÁùÄÂõ∫ÂÆöÔºè‰∫∫Ê∞óÔºç„É©„É≥„ÇØÔºâÔºö{', '.join(pretty) if pretty else '‚Äî'}",
           f"  ‚Üí ÂÄôË£ú {n}È†≠ÔºèÂêàË®à {pts}ÁÇπ"
    ]
    if odds_timestamp_hm: lines += [f"\nüìÖ „Ç™„ÉÉ„Ç∫ÊôÇÁÇπ: {odds_timestamp_hm}"]
    lines += ["üîó „Ç™„ÉÉ„Ç∫Ë©≥Á¥∞:", odds_url, ""]
    lines += [
        "‚Äª„Ç™„ÉÉ„Ç∫„ÅØÁ∑†ÂàáÁõ¥Ââç„Åæ„ÅßÂ§âÂãï„Åó„Åæ„Åô„ÄÇ",
        "‚ÄªÈ®éÊâã„É©„É≥„ÇØ„ÅØ2024Âπ¥„ÅÆÂú∞ÊñπÁ´∂È¶¨„É™„Éº„Éá„Ç£„É≥„Ç∞„Ç∏„Éß„ÉÉ„Ç≠„Éº„É©„É≥„Ç≠„É≥„Ç∞„Å´Âü∫„Å•„Åç„ÄÅA=1„Äú70‰Ωç / B=71„Äú200‰Ωç / C=„Åù„ÅÆ‰ªñ„ÄÇ",
        "‚ÄªÈ¶¨Âà∏Ë≥ºÂÖ•„ÅØ‰ΩôË£ïË≥áÈáë„Åß„ÄÇÁöÑ‰∏≠„ÅØ‰øùË®º„Åï„Çå„Åæ„Åõ„Çì„ÄÇ"
    ]
    return "\n".join(lines)

# ========= „Éô„ÉÉ„ÉàË®òÈå≤ =========
def _bets_sheet_header() -> List[str]:
    return ["date","race_id","venue","race_no","strategy","bet_kind","tickets_umaban_csv","points","unit_stake","total_stake"]

def sheet_append_bet_record(date_ymd:str, race_id:str, venue:str, race_no:str,
                            strategy_no:int, bet_kind:str, tickets_umaban:List[str]):
    svc=_sheet_service(); title=_resolve_sheet_title(svc, BETS_SHEET_TAB)
    values=_sheet_get_range_values(svc, title, "A:J")
    if not values: values=[_bets_sheet_header()]
    points=len(tickets_umaban); unit=UNIT_STAKE_YEN; total=points*unit
    values.append([date_ymd, race_id, venue, race_no, str(strategy_no), bet_kind, ",".join(tickets_umaban), str(points), str(unit), str(total)])
    _sheet_update_range_values(svc, title, "A:J", values)

# ========= ÊâïÊàªÂèñÂæóÔºÜÊó•Ê¨°„Çµ„Éû„É™ =========
_PAYOUT_KIND_KEYS = ["ÂçòÂãù","Ë§áÂãù","Êû†ÈÄ£","È¶¨ÈÄ£","„ÉØ„Ç§„Éâ","È¶¨Âçò","‰∏âÈÄ£Ë§á","‰∏âÈÄ£Âçò"]

def fetch_payoff_map(race_id:str) -> Dict[str, List[Tuple[str,int]]]:
    url=f"https://keiba.rakuten.co.jp/race/payoff/RACEID/{race_id}"
    html=fetch(url); soup=BeautifulSoup(html,"lxml")
    result={}
    for kind in _PAYOUT_KIND_KEYS:
        blocks=soup.find_all(string=re.compile(kind))
        items=[]
        for b in blocks:
            box=getattr(b,"parent",None) or soup
            text=" ".join((box.get_text(" ", strip=True) or "").split())
            for m in re.finditer(r"(\d+(?:-\d+){0,2})\s*([\d,]+)\s*ÂÜÜ", text):
                comb=m.group(1); pay=int(m.group(2).replace(",",""))
                items.append((comb, pay))
        if items: result[kind]=items
    return result

def _normalize_ticket_for_kind(ticket:str, kind:str) -> str:
    parts=[int(x) for x in ticket.split("-") if x.strip().isdigit()]
    if kind in ("È¶¨ÈÄ£","‰∏âÈÄ£Ë§á"): parts=sorted(parts)
    return "-".join(str(x) for x in parts)

def _summary_key_for_today() -> str:
    return f"summary:{now_jst():%Y%m%d}"

def _is_time_reached(now: datetime, hhmm: str) -> bool:
    try: hh,mm=map(int, hhmm.split(":"))
    except Exception: return False
    target=now.replace(hour=hh, minute=mm, second=0, microsecond=0)
    return now >= target

def summarize_today_and_notify(targets: List[str]):
    svc=_sheet_service(); title=_resolve_sheet_title(svc, BETS_SHEET_TAB)
    values=_sheet_get_range_values(svc, title, "A:J")
    if not values or values==[_bets_sheet_header()]:
        if not ALWAYS_NOTIFY_DAILY_SUMMARY:
            logging.info("[INFO] bets„Ç∑„Éº„Éà„Å´ÂΩìÊó•„Éá„Éº„Çø„Å™„ÅóÔºàÁÑ°ÈÄöÁü•„É¢„Éº„ÉâÔºâ"); return
        values=[_bets_sheet_header()]
    hdr=values[0]; rows=values[1:]
    today=now_jst().strftime("%Y%m%d")
    records=[r for r in rows if len(r)>=10 and r[0]==today]
    if not records and not ALWAYS_NOTIFY_DAILY_SUMMARY:
        logging.info("[INFO] ÂΩìÊó•ÂàÜ„Å™„ÅóÔºàÁÑ°ÈÄöÁü•„É¢„Éº„ÉâÔºâ"); return

    per_strategy = { k:{"races":0,"hits":0,"bets":0,"stake":0,"return":0} for k in ("1","2","3","4") }
    seen_race_strategy:set[Tuple[str,str]] = set()

    for r in records:
        date_ymd, race_id, venue, race_no, strategy, bet_kind, t_csv, points, unit, total = r[:10]
        if (race_id, strategy) not in seen_race_strategy:
            seen_race_strategy.add((race_id, strategy))
            per_strategy[strategy]["races"] += 1
        tickets=[t for t in t_csv.split(",") if t]
        per_strategy[strategy]["bets"]  += len(tickets)
        per_strategy[strategy]["stake"] += int(total)

        paymap=fetch_payoff_map(race_id)
        winners={ _normalize_ticket_for_kind(comb, bet_kind): pay for (comb, pay) in paymap.get(bet_kind, []) }
        for t in tickets:
            norm=_normalize_ticket_for_kind(t, bet_kind)
            if norm in winners:
                per_strategy[strategy]["hits"]   += 1
                per_strategy[strategy]["return"] += winners[norm]
        time.sleep(0.2)

    total_stake=sum(v["stake"] for v in per_strategy.values())
    total_return=sum(v["return"] for v in per_strategy.values())
    def pct(n,d): return f"{(100.0*n/d):.1f}%" if d>0 else "0.0%"

    lines=["üìä„ÄêÊú¨Êó•„ÅÆÊ§úË®ºÁµêÊûú„Äë", f"Êó•‰ªòÔºö{today[:4]}/{today[4:6]}/{today[6:]}", ""]
    for k in ("1","2","3","4"):
        v=per_strategy[k]
        hit_rate=pct(v["hits"], max(v["bets"],1))
        roi=pct(v["return"], max(v["stake"],1))
        lines.append(f"Êà¶Áï•{k}ÔºöË©≤ÂΩì{v['races']}„É¨„Éº„Çπ / Ë≥ºÂÖ•{v['bets']}ÁÇπ / ÁöÑ‰∏≠{v['hits']}ÁÇπ")
        lines.append(f"„ÄÄ„ÄÄ„ÄÄÁöÑ‰∏≠Áéá {hit_rate} / ÂõûÂèéÁéá {roi}")
    lines.append("")
    lines.append(f"ÂêàË®àÔºöÊäïË≥á {total_stake:,}ÂÜÜ / ÊâïÊàª {total_return:,}ÂÜÜ / ÂõûÂèéÁéá {pct(total_return, max(total_stake,1))}")
    notify_strategy_hit_to_many("\n".join(lines), targets)

# ========= Áõ£Ë¶ñÊú¨‰ΩìÔºà‰∏ÄÂõûÂÆüË°åÔºâ =========
def _tickets_pop_to_umaban(bets: List[str], horses: List[Dict[str,float]]) -> List[str]:
    pop2=_map_pop_info(horses); out=[]
    for b in bets:
        pops=[int(x) for x in re.findall(r"\d+", str(b))]
        if not pops: out.append(b); continue
        nums=[]; ok=True
        for p in pops:
            n=pop2.get(p,{}).get("umaban")
            if n is None: ok=False; break
            nums.append(str(n))
        out.append("-".join(nums) if ok else b)
    return out

def main():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    p=pathlib.Path(__file__).resolve()
    sha=hashlib.sha1(p.read_bytes()).hexdigest()[:12]
    logging.info(f"[BUILD] file={p} sha1={sha} v2025-08-15B")

    if KILL_SWITCH:
        logging.info("[INFO] KILL_SWITCH=True"); return

    # ÈÄÅ‰ø°„Çø„Éº„Ç≤„ÉÉ„Éà
    try:
        targets=load_user_ids_from_simple_col()
        if not targets:
            fb=LINE_USER_IDS if LINE_USER_IDS else ([LINE_USER_ID] if LINE_USER_ID else [])
            targets=fb
    except Exception as e:
        logging.exception("[ERROR] users„Ç∑„Éº„ÉàË™≠ËæºÂ§±Êïó: %s", e)
        fb=LINE_USER_IDS if LINE_USER_IDS else ([LINE_USER_ID] if LINE_USER_ID else [])
        targets=fb
    logging.info("[INFO] ÈÄÅ‰ø°ÂÖà=%d", len(targets))

    # Á®ºÂÉçÊôÇÈñìÂÜÖ„ÅßÈÄöÂ∏∏Áõ£Ë¶ñ
    if within_operating_hours():
        try:
            notified=sheet_load_notified()
        except Exception as e:
            logging.exception("[ERROR] TTL„É≠„Éº„ÉâÂ§±Êïó: %s", e)
            notified={}
        if DEBUG_RACEIDS:
            target_raceids=[rid for rid in DEBUG_RACEIDS if not PLACEHOLDER.search(rid)]
            post_time_map={}
        else:
            ymd=now_jst().strftime("%Y%m%d")
            ymd_next=(now_jst()+timedelta(days=1)).strftime("%Y%m%d")
            r1=list_raceids_today_ticket(ymd)
            r2=list_raceids_from_card_lists(ymd, ymd_next)
            target_raceids=sorted(set(r1)|set(r2))
            post_time_map=collect_post_time_map(ymd, ymd_next)
            target_raceids=[rid for rid in target_raceids if not PLACEHOLDER.search(rid)]

        hits=0; matches=0
        seen_in_this_run:set[str]=set()

        for rid in target_raceids:
            if rid in seen_in_this_run: continue
            now_ts=time.time()
            cd_ts=notified.get(f"{rid}:cd")
            if cd_ts and (now_ts-cd_ts)<NOTIFY_COOLDOWN_SEC: continue
            ts=notified.get(rid)
            if ts and (now_ts-ts)<NOTIFY_TTL_SEC: continue

            # Áô∫Ëµ∞ÔºàÂü∫Ê∫ñÔºâÂèñÂæó
            post_time=post_time_map.get(rid)
            if not post_time:
                got=fallback_post_time_for_rid(rid)
                if got: post_time, _, _ = got
                else: 
                    logging.info("[TRACE] time rid=%s result=SKIP reason=no_post_time", rid)
                    continue

            # Á∑†ÂàáÂèñÂæóÔºàÊúÄÂÑ™ÂÖàÔºâ
            cutoff_info=resolve_cutoff_dt(rid) if CUTOFF_OFFSET_MIN>0 else None
            if cutoff_info:
                cutoff_dt, cutoff_src = cutoff_info
                used_dt = cutoff_dt
                time_label = "Á∑†Âàá"
                src_label  = f"cutoff:{cutoff_src}"
            else:
                # ‰ª£Áî®ÔºöÁô∫Ëµ∞ - CUTOFF_OFFSET_MIN
                used_dt = post_time - timedelta(minutes=CUTOFF_OFFSET_MIN) if CUTOFF_OFFSET_MIN>0 else post_time
                time_label = "Á∑†Âàá" if CUTOFF_OFFSET_MIN>0 else "Áô∫Ëµ∞"
                src_label  = "post-offset" if CUTOFF_OFFSET_MIN>0 else "post"

            now=now_jst()
            in_win=is_within_window(used_dt, now)
            logging.info("[TRACE] time rid=%s use=%s src=%s at=%s target=%s Œîsec=%.1f in_window=%s",
                         rid, time_label, src_label, now.strftime("%H:%M:%S"), used_dt.strftime("%H:%M"),
                         (used_dt-now).total_seconds(), in_win)
            if not in_win: continue

            meta=check_tanfuku_page(rid)
            if not meta:
                logging.info("[TRACE] odds rid=%s result=SKIP reason=no_table", rid)
                time.sleep(random.uniform(*SLEEP_BETWEEN)); continue

            horses=meta["horses"]
            if len(horses)<4:
                logging.info("[TRACE] odds rid=%s result=SKIP reason=too_few_horses len=%d", rid, len(horses))
                time.sleep(random.uniform(*SLEEP_BETWEEN)); continue

            # „Ç™„ÉÉ„Ç∫„Çπ„Éä„ÉÉ„ÉóÔºàÂÆâÂÖ®ÂåñÔºöÊã¨ÂºßÂØæÂøúÔºâ
            top3=sorted(horses, key=lambda x:int(x.get("pop",999)))[:3]
            snapshot = [(int(h.get("pop", 0)), float(h.get("odds", 0.0))) for h in top3 if "pop" in h and "odds" in h]
            logging.info("[TRACE] odds_top3 rid=%s %s", rid, snapshot)

            hits+=1
            strategy=eval_strategy(horses, logger=logging)
            if not strategy:
                logging.info("[TRACE] judge rid=%s result=FAIL reason=no_strategy_match", rid)
                time.sleep(random.uniform(*SLEEP_BETWEEN)); continue
            matches+=1
            logging.info("[TRACE] judge rid=%s result=PASS strat=%s", rid, strategy.get("strategy",""))

            strategy_text=strategy.get("strategy","")
            pattern_no=_infer_pattern_no(strategy_text)
            condition_text=_strip_pattern_prefix(strategy_text) or strategy_text

            venue_disp, race_no=_split_venue_race(meta.get("venue_race",""))
            display_dt=used_dt
            time_hm=display_dt.strftime("%H:%M")
            odds_hm=_extract_hhmm_label(meta.get("now",""))

            raw_tickets=strategy.get("tickets", [])
            if isinstance(raw_tickets, str):
                raw_tickets=[s.strip() for s in raw_tickets.split(",") if s.strip()]

            # ÈÄöÁü•Êú¨Êñá
            if str(strategy_text).startswith("‚ë¢"):
                message=build_line_notification_strategy3(strategy, venue_disp, race_no, time_label, time_hm, odds_hm, meta["url"], horses)
                tickets_umaban = strategy.get("tickets", [])  # ‚ë¢„ÅØumabanÁîüÊàê„ÅÆ„Åì„Å®„ÅåÂ§ö„ÅÑ
                bet_kind = STRATEGY_BET_KIND.get("3", "‰∏âÈÄ£Âçò")
            else:
                message=build_line_notification(pattern_no, venue_disp, race_no, time_label, time_hm, condition_text, raw_tickets, odds_hm, meta["url"], horses)
                tickets_umaban=_tickets_pop_to_umaban(raw_tickets, horses)
                bet_kind = STRATEGY_BET_KIND.get(str(pattern_no), "‰∏âÈÄ£Âçò")

            # ÈÄÅ‰ø°
            sent_ok, http_status = notify_strategy_hit_to_many(message, targets)

            # ÈÄöÁü•„É≠„Ç∞ÔºàÈÄÅ‰ø°ÊàêÂäüÊôÇ„ÅÆ„ÅøÔºâ
            if sent_ok:
                try:
                    append_notify_log({
                        'date_jst': jst_today_str(),
                        'race_id': rid,
                        'strategy': str(pattern_no),
                        'stake': len(tickets_umaban) * UNIT_STAKE_YEN,
                        'bets_json': json.dumps(tickets_umaban, ensure_ascii=False),
                        'notified_at': jst_now(),
                        'jockey_ranks': "/".join([
                            jockey_rank_letter_by_name(h.get("jockey")) if h.get("jockey") else "‚Äî"
                            for h in horses[:3]
                        ]),
                    })
                except Exception as e:
                    logging.exception("[WARN] append_notify_logÂ§±Êïó: %s", e)

            now_epoch=time.time()
            if sent_ok:
                try:
                    sheet_upsert_notified(rid, now_epoch, note=f"{meta['venue_race']} {display_dt:%H:%M} {src_label}")
                except Exception as e:
                    logging.exception("[ERROR] TTLÊõ¥Êñ∞Â§±Êïó: %s", e)
                seen_in_this_run.add(rid)
                try:
                    ymd=now_jst().strftime("%Y%m%d")
                    sheet_append_bet_record(ymd, rid, venue_disp, race_no, pattern_no, bet_kind, tickets_umaban or [])
                except Exception as e:
                    logging.exception("[ERROR] betsË®òÈå≤Â§±Êïó: %s", e)
            elif http_status==429:
                try:
                    key_cd=f"{rid}:cd"; sheet_upsert_notified(key_cd, now_epoch, note=f"429 cooldown {meta['venue_race']} {display_dt:%H:%M}")
                except Exception as e:
                    logging.exception("[ERROR] CDÊõ¥Êñ∞Â§±Êïó: %s", e)

            time.sleep(random.uniform(*SLEEP_BETWEEN))

        logging.info(f"[INFO] HITS={hits} / MATCHES={matches}")

    # === Êó•Ê¨°„Çµ„Éû„É™ÔºöÊåáÂÆöÊôÇÂàª„Å´1Êó•1Âõû ===
    try:
        now = now_jst()
        if _is_time_reached(now, DAILY_SUMMARY_HHMM):
            notified = {}
            try:
                notified = sheet_load_notified()
            except Exception:
                pass
            skey = _summary_key_for_today()
            if skey not in notified:
                summarize_today_and_notify(targets)
                try:
                    sheet_upsert_notified(skey, time.time(), note=f"daily summary {now:%H:%M}")
                except Exception as e:
                    logging.exception("[ERROR] „Çµ„Éû„É™ÈÄÅ‰ø°„Éï„É©„Ç∞„ÅÆ‰øùÂ≠ò„Å´Â§±Êïó: %s", e)
    except Exception as e:
        logging.exception("[ERROR] Êó•Ê¨°„Çµ„Éû„É™ÈÄÅ‰ø°Âà§ÂÆö„Å´Â§±Êïó: %s", e)

    logging.info("[INFO] „Ç∏„Éß„ÉñÁµÇ‰∫Ü")

# ========= Â∏∏Èßê„É´„Éº„Éó =========
def run_watcher_forever(interval_sec: int = int(os.getenv("WATCHER_INTERVAL_SEC", "60"))):
    logging.info(f"[BOOT] run_watcher_forever(interval={interval_sec}s)")
    while True:
        try:
            main()
        except Exception as e:
            logging.exception("[FATAL] watcher„É´„Éº„Éó‰æãÂ§ñ: %s", e)
        time.sleep(interval_sec)