# -*- coding: utf-8 -*-
"""
Rakutenç«¶é¦¬ ç›£è¦–ãƒ»é€šçŸ¥ãƒãƒƒãƒï¼ˆå®Œå…¨å·®ã—æ›¿ãˆç‰ˆ v2025-08-15Eï¼‰
- ç· åˆ‡æ™‚åˆ»ï¼šå˜è¤‡ã‚ªãƒƒã‚º/ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰â€œç· åˆ‡â€ã‚’ç›´æ¥æŠ½å‡ºï¼ˆæœ€å„ªå…ˆï¼‰
- ç™ºèµ°æ™‚åˆ»ï¼šä¸€è¦§ãƒšãƒ¼ã‚¸å„ªå…ˆï¼‹ã‚ªãƒƒã‚ºè©³ç´°ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
- çª“åˆ¤å®šï¼šã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ™‚åˆ»ï¼ˆç· åˆ‡ or ç™ºèµ°ï¼‰åŸºæº–ã€Â±GRACE_SECONDS ã®è¨±å®¹
- é€šçŸ¥ï¼šçª“å†…1å› / 429æ™‚ã¯ã‚¯ãƒ¼ãƒ«ãƒ€ã‚¦ãƒ³ / Google Sheetã§TTLæ°¸ç¶š
- é€ä¿¡å…ˆï¼šGoogleã‚·ãƒ¼ãƒˆ(ã‚¿ãƒ–A=åç§°ã€Œ1ã€)ã®Håˆ—ã‹ã‚‰ userId ã‚’åé›†
- æˆ¦ç•¥â‘¢ï¼šå°‚ç”¨ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆ1è»¸ãƒ»ç›¸æ‰‹10ã€œ20å€ãƒ»å€™è£œæœ€å¤§4é ­ãƒ»ç‚¹æ•°è¡¨ç¤ºï¼‰
- é¨æ‰‹ãƒ©ãƒ³ã‚¯ï¼šå†…è”µ200ä½ï¼‹è¡¨è¨˜ã‚†ã‚Œè€æ€§ï¼ˆå¼·åŒ–ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ï¼‹å‰æ–¹ä¸€è‡´ï¼‹å§“ä¸€è‡´ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰
- â˜…é€šçŸ¥æœ¬æ–‡ï¼šè²·ã„ç›®ã‚’ã€Œé¦¬ç•ªï¼‹ã‚ªãƒƒã‚ºï¼‹é¨æ‰‹ãƒ©ãƒ³ã‚¯ã€ã§è¡¨ç¤º
  ä¾‹ï¼š3ç•ªï¼ˆ1äººæ°—ï¼1.7å€ï¼Aï¼‰- 5ç•ªï¼ˆ3äººæ°—ï¼6.0å€ï¼Bï¼‰
  â€»æˆ¦ç•¥1/2/4ã®è¡¨ç¤ºã¯ã€äººæ°—å„ªå…ˆâ†’é¦¬ç•ªã¸å¤‰æ›ã€ã«çµ±ä¸€ï¼ˆ11äººæ°—æ··å…¥å¯¾ç­–ï¼‰
- æœªä¸€è‡´ã®é¨æ‰‹åã¯ [RANKMISS] ãƒ­ã‚°ã«è¨˜éŒ²ï¼ˆé‡è¤‡æŠ‘æ­¢ï¼‰ï¼‹ [RANKDBG] ã§çªåˆéç¨‹ã‚’å‡ºåŠ›
- betsã‚·ãƒ¼ãƒˆï¼šé¦¬ç•ªãƒ™ãƒ¼ã‚¹ã§è¨˜éŒ²ï¼ˆä»•æ§˜ã¯å¾“æ¥é€šã‚Šï¼‰
- æ—¥æ¬¡ã‚µãƒãƒªï¼šJST 21:02 ã«å½“æ—¥1å›ã ã‘é€ä¿¡ï¼ˆ0ä»¶ã§ã‚‚é€ä¿¡ï¼‰ â† FIX
- åˆ¸ç¨®ã¯ STRATEGY_BET_KIND_JSON ã§è¨­å®šï¼ˆæ—¢å®š: â‘ é¦¬é€£, â‘¡é¦¬å˜, â‘¢ä¸‰é€£å˜, â‘£ä¸‰é€£è¤‡ï¼‰

â˜…ç· åˆ‡åŸºæº–ã§é‹ç”¨ã™ã‚‹å ´åˆï¼š
  - ç’°å¢ƒå¤‰æ•° CUTOFF_OFFSET_MIN ã‚’ 5ï¼ˆæ¨å¥¨ï¼‰ã«è¨­å®š
  - æœ¬ç‰ˆã¯ â€œç· åˆ‡ãã®ã‚‚ã®â€ ã‚’æŠ½å‡ºã§ããŸã‚‰ãã‚Œã‚’æ¡ç”¨ã€‚å–ã‚Œãªã„å ´åˆã®ã¿ã€Œç™ºèµ°-5åˆ†ã€ã‚’ä»£ç”¨ã€‚

â˜…æœ¬ç‰ˆã®å¤‰æ›´ç‚¹ï¼ˆEï¼‰ï¼š
  - æ—¥æ¬¡ã‚µãƒãƒªå®Ÿè¡Œã‚’ 21:02 ã§ç¢ºå®Ÿã«ãƒˆãƒªã‚¬ï¼ˆ>= åˆ¤å®šãƒ»é‡è¤‡æŠ‘æ­¢ã¯ã‚·ãƒ¼ãƒˆã® summary ãƒ•ãƒ©ã‚°ï¼‰
  - ã‚µãƒãƒªä½œæˆä¸­ã®æ‰•æˆ»ãƒšãƒ¼ã‚¸404ç­‰ã¯ãƒ¬ãƒ¼ã‚¹å˜ä½ã§æ•æ‰ã—ã¦ã‚¹ã‚­ãƒƒãƒ—ï¼ˆå‡¦ç†å…¨ä½“ã‚’è½ã¨ã•ãªã„ï¼‰
  - ã‚µãƒãƒªé–‹å§‹ãƒ»çµæœãƒ»ã‚¹ã‚­ãƒƒãƒ—ç†ç”±ãªã©ãƒ­ã‚°ã‚’å¼·åŒ–
"""

import os, re, json, time, random, logging, pathlib, hashlib, unicodedata
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Optional, Tuple, Set

import requests
from bs4 import BeautifulSoup, Tag
from strategy_rules import eval_strategy

# --- é€šçŸ¥ãƒ­ã‚° append ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ã import ---
try:
    from utils_notify_log import append_notify_log
except ModuleNotFoundError:
    import logging as _logging
    def append_notify_log(*args, **kwargs):
        _logging.warning("[WARN] utils_notify_log ãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€é€šçŸ¥ãƒ­ã‚°ã®è¿½è¨˜ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")

# æ—¥ä»˜ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
from utils_summary import jst_today_str, jst_now

# ===== Google Sheets =====
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build

# ========= åŸºæœ¬è¨­å®š =========
JST = timezone(timedelta(hours=9))
SESSION = requests.Session()
SESSION.headers.update({
    "User-Agent": ("Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                   "AppleWebKit/537.36 (KHTML, like Gecko) "
                   "Chrome/124.0.0.0 Safari/537.36"),
    "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
    "Cache-Control": "no-cache",
})
TIMEOUT = (10, 25)
RETRY = 3
SLEEP_BETWEEN = (0.6, 1.2)

LINE_PUSH_URL = "https://api.line.me/v2/bot/message/push"

# ========= ç’°å¢ƒå¤‰æ•° =========
START_HOUR          = int(os.getenv("START_HOUR", "10"))
END_HOUR            = int(os.getenv("END_HOUR",   "22"))
DRY_RUN             = os.getenv("DRY_RUN", "False").lower() == "true"
KILL_SWITCH         = os.getenv("KILL_SWITCH", "False").lower() == "true"
NOTIFY_ENABLED      = os.getenv("NOTIFY_ENABLED", "1") == "1"
DEBUG_RACEIDS       = [s.strip() for s in os.getenv("DEBUG_RACEIDS", "").split(",") if s.strip()]

NOTIFY_TTL_SEC      = int(os.getenv("NOTIFY_TTL_SEC", "3600"))
NOTIFY_COOLDOWN_SEC = int(os.getenv("NOTIFY_COOLDOWN_SEC", "1800"))

WINDOW_BEFORE_MIN   = int(os.getenv("WINDOW_BEFORE_MIN", "15"))
WINDOW_AFTER_MIN    = int(os.getenv("WINDOW_AFTER_MIN", "0"))   # ç· åˆ‡é‹ç”¨ãªã‚‰ 0 æ¨å¥¨
CUTOFF_OFFSET_MIN   = int(os.getenv("CUTOFF_OFFSET_MIN", "0"))  # ä¾‹: 5ï¼ˆç· åˆ‡ç›´æ¥å–å¾—ã§ããªã„å ´åˆã®ä»£ç”¨ï¼‰
FORCE_RUN           = os.getenv("FORCE_RUN", "0") == "1"
GRACE_SECONDS       = int(os.getenv("GRACE_SECONDS", "60"))     # å¢ƒç•Œè¨±å®¹ï¼ˆç§’ï¼‰

LINE_ACCESS_TOKEN   = os.getenv("LINE_ACCESS_TOKEN", "")
LINE_USER_ID        = os.getenv("LINE_USER_ID", "")
LINE_USER_IDS       = [s.strip() for s in os.getenv("LINE_USER_IDS", "").split(",") if s.strip()]

GOOGLE_CREDENTIALS_JSON = os.getenv("GOOGLE_CREDENTIALS_JSON", "")
GOOGLE_SHEET_ID         = os.getenv("GOOGLE_SHEET_ID", "")

# TTLç®¡ç†ã‚¿ãƒ–ï¼ˆåå‰ or gidï¼‰
GOOGLE_SHEET_TAB        = os.getenv("GOOGLE_SHEET_TAB", "notified")

# é€ä¿¡å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚’èª­ã‚€ã‚¿ãƒ–Aï¼ˆ=ã€Œ1ã€ï¼‰ã¨åˆ—ï¼ˆ=Hï¼‰
USERS_SHEET_NAME        = os.getenv("USERS_SHEET_NAME", "1")
USERS_USERID_COL        = os.getenv("USERS_USERID_COL", "H")

# ãƒ™ãƒƒãƒˆè¨˜éŒ²ã‚¿ãƒ–
BETS_SHEET_TAB          = os.getenv("BETS_SHEET_TAB", "bets")

# åˆ¸ç¨®ï¼ˆæˆ¦ç•¥â†’åˆ¸ç¨®ï¼‰
_DEFAULT_BET_KIND = {"1":"é¦¬é€£", "2":"é¦¬å˜", "3":"ä¸‰é€£å˜", "4":"ä¸‰é€£è¤‡"}
try:
    STRATEGY_BET_KIND = json.loads(os.getenv("STRATEGY_BET_KIND_JSON","")) or _DEFAULT_BET_KIND
except Exception:
    STRATEGY_BET_KIND = _DEFAULT_BET_KIND

UNIT_STAKE_YEN = int(os.getenv("UNIT_STAKE_YEN", "100"))  # 1ç‚¹100å††

# === æ—¥æ¬¡ã‚µãƒãƒª ===
DAILY_SUMMARY_HHMM = os.getenv("DAILY_SUMMARY_HHMM", "21:02")  # æ±ºã¾ã£ãŸæ™‚åˆ»ã«1å›é€ã‚‹ï¼ˆJSTï¼‰
ALWAYS_NOTIFY_DAILY_SUMMARY = os.getenv("ALWAYS_NOTIFY_DAILY_SUMMARY", "1") == "1"  # 0ä»¶ã§ã‚‚é€ã‚‹

RACEID_RE   = re.compile(r"/RACEID/(\d{18})")
TIME_PATS = [
    re.compile(r"\b(\d{1,2}):(\d{2})\b"),
    re.compile(r"\b(\d{1,2})ï¼š(\d{2})\b"),
    re.compile(r"\b(\d{1,2})\s*æ™‚\s*(\d{1,2})\s*åˆ†\b"),
]
PLACEHOLDER = re.compile(r"\d{8}0000000000$")

# ãƒ©ãƒ™ãƒ«é¡
IGNORE_NEAR_PAT   = re.compile(r"(ç¾åœ¨|æ›´æ–°|ç™ºå£²|ç¢ºå®š|æ‰•æˆ»|å®Ÿæ³)")
POST_LABEL_PAT    = re.compile(r"(ç™ºèµ°|ç™ºèµ°äºˆå®š|ç™ºèµ°æ™‚åˆ»|ç™ºé€|å‡ºèµ°)")
CUTOFF_LABEL_PAT  = re.compile(r"(æŠ•ç¥¨ç· åˆ‡|ç™ºå£²ç· åˆ‡|ç· åˆ‡)")

# ========= é¨æ‰‹ãƒ©ãƒ³ã‚¯ï¼ˆ1ã€œ200ä½ã‚’å†…è”µï¼‰ =========
_RANKMISS_SEEN: Set[str] = set()

def _log_rank_miss(orig: str, norm: str):
    key = f"{orig}|{norm}"
    if key not in _RANKMISS_SEEN:
        _RANKMISS_SEEN.add(key)
        logging.info("[RANKMISS] name_raw=%s name_norm=%s", orig, norm)

def _normalize_name(s: str) -> str:
    """å…¨åŠè§’æ­£è¦åŒ–ãƒ»ç©ºç™½é™¤å»ãƒ»æ—§å­—ä½“/ç•°ä½“å­—ã®ä»£è¡¨è¡¨è¨˜åŒ–"""
    if not s: return ""
    s = unicodedata.normalize("NFKC", s)
    s = s.replace(" ", "").replace("\u3000", "")
    replace_map = {
        "ğ ®·": "å‰", "æ ": "æŸ³", "é«™": "é«˜", "æ¿µ": "æµœ", "ï¨‘": "å´", "å¶‹": "å³¶", "å³¯": "å³°",
        "é½‹": "æ–", "é½Š": "æ–‰", "å…§": "å†…", "å†¨": "å¯Œ", "åœ‹": "å›½", "é«”": "ä½“", "çœ": "çœŸ",
        "å»£": "åºƒ", "é‚Š": "è¾º", "é‚‰": "è¾º", "æ¸¡é‚Š": "æ¸¡è¾º", "æ¸¡é‚‰": "æ¸¡è¾º",
    }
    for k, v in replace_map.items():
        s = s.replace(k, v)
    return s

def _clean_jockey_name(s: str) -> str:
    """æ‹¬å¼§/æ–¤é‡/å°/æ¥å°¾è¾(J/ï¼ª/é¨æ‰‹)ã‚’é™¤å»ã—ç´ ã®æ°åã¸"""
    if not s: return ""
    s = re.sub(r"[ï¼ˆ(].*?[ï¼‰)]", "", s)                           # æ‹¬å¼§å†…
    s = re.sub(r"[â–²â–³â˜†â˜…â—‡â—†âŠ™â—â—‹â—¯â—‰âšªï¸ï¼‹+ï¼Š*]", "", s)                # å°
    s = re.sub(r"\d+(?:\.\d+)?\s*(?:kg|æ–¤)?", "", s)               # æ–¤é‡
    s = s.replace("æ–¤é‡", "")
    s = s.replace("é¨æ‰‹", "").replace("J", "").replace("ï¼ª", "")   # æ¥å°¾è¾
    s = re.sub(r"\s+", "", s)
    return s

# 1ã€œ200ä½ãƒ©ãƒ³ã‚¯è¡¨ï¼ˆæŠœç²‹å…ˆé ­ã€‚â€»é‡ç•‘å‡Œ=ãƒ©ãƒ³ã‚¯6 â†’ Aï¼‰
JOCKEY_RANK_TABLE_RAW: Dict[int, str] = json.loads(os.getenv("JOCKEY_RANKS_JSON", "{}"))
JOCKEY_RANK_TABLE_RAW: Dict[int, str] = {
_JOCKEY_NAME_TO_RANK: Dict[str, int] = { _normalize_name(v): k for k, v in JOCKEY_RANK_TABLE_RAW.items() }

def _split_family_given(n: str) -> Tuple[str, str]:
    """å§“ãƒ»åï¼ˆåã¯é€£çµï¼‰ã‚’è¿”ã™ã€‚ç©ºç™½ãŒç„¡ã‘ã‚Œã°å…¨ä½“ã‚’å§“ã¨ã—ã¦æ‰±ã†ã€‚"""
    if not n: return "", ""
    parts = re.split(r"[\s\u3000]", n)
    if len(parts) >= 2:
        return parts[0], "".join(parts[1:])
    return n, ""

def _best_match_rank(name_norm: str) -> Optional[int]:
    """
    ç›´æ¥ä¸€è‡´ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼š
      1) å‰æ–¹ä¸€è‡´/é€†å‰æ–¹ä¸€è‡´
      2) å§“å®Œå…¨ä¸€è‡´ï¼‹åé ­æ–‡å­—ä¸€è‡´
      3) å§“å®Œå…¨ä¸€è‡´
      â†’ tie ã¯ãƒ©ãƒ³ã‚¯ä¸Šä½ã‚’å„ªå…ˆ
    """
    cands=[]
    fam, given = _split_family_given(name_norm)
    for n2, rank in _JOCKEY_NAME_TO_RANK.items():
        if n2.startswith(name_norm) or name_norm.startswith(n2):
            cands.append((0, rank)); continue
        f2, g2 = _split_family_given(n2)
        if fam and fam == f2:
            if given and g2 and given[0] == g2[0]:
                cands.append((1, rank))
            else:
                cands.append((2, rank))
    if not cands: return None
    cands.sort(key=lambda x:(x[0], x[1]))
    return cands[0][1]

def jockey_rank_letter_by_name(name: Optional[str]) -> str:
    """è¡¨ç¤ºãƒ©ãƒ³ã‚¯: A=1ã€œ70 / B=71ã€œ200 / C=ãã®ä»– / â€”=åå‰ãªã—"""
    if not name: return "â€”"
    base_raw = _clean_jockey_name(name)
    base = _normalize_name(base_raw)
    rank = _JOCKEY_NAME_TO_RANK.get(base)
    if rank is None and base:
        rank = _best_match_rank(base)
    if rank is None:
        _log_rank_miss(base_raw, base)
        return "C"
    return "A" if 1<=rank<=70 else ("B" if 71<=rank<=200 else "C")

# ========= å…±é€š =========
def now_jst() -> datetime: return datetime.now(JST)

def within_operating_hours() -> bool:
    if FORCE_RUN: return True
    return START_HOUR <= now_jst().hour < END_HOUR

def fetch(url: str) -> str:
    last_err=None
    for i in range(1, RETRY+1):
        try:
            r=SESSION.get(url, timeout=TIMEOUT); r.raise_for_status()
            r.encoding="utf-8"; return r.text
        except Exception as e:
            last_err=e; wait=random.uniform(*SLEEP_BETWEEN)
            logging.warning(f"[WARN] fetchå¤±æ•—({i}/{RETRY}) {e} -> {wait:.1f}så¾…æ©Ÿ: {url}")
            time.sleep(wait)
    raise last_err

# ========= Google Sheets =========
def _sheet_service():
    if not GOOGLE_CREDENTIALS_JSON or not GOOGLE_SHEET_ID:
        raise RuntimeError("Google Sheets ã®ç’°å¢ƒå¤‰æ•°ä¸è¶³")
    info=json.loads(GOOGLE_CREDENTIALS_JSON)
    creds=Credentials.from_service_account_info(info, scopes=["https://www.googleapis.com/auth/spreadsheets"])
    return build("sheets","v4",credentials=creds, cache_discovery=False)

def _resolve_sheet_title(svc, tab_or_gid: str) -> str:
    meta=svc.spreadsheets().get(spreadsheetId=GOOGLE_SHEET_ID).execute()
    sheets=meta.get("sheets", [])
    if tab_or_gid.isdigit() and len(tab_or_gid)>3:
        gid=int(tab_or_gid)
        for s in sheets:
            if s["properties"]["sheetId"]==gid:
                return s["properties"]["title"]
        raise RuntimeError(f"æŒ‡å®šgidã®ã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {gid}")
    for s in sheets:
        if s["properties"]["title"]==tab_or_gid:
            return tab_or_gid
    body={"requests":[{"addSheet":{"properties":{"title":tab_or_gid}}}]}
    svc.spreadsheets().batchUpdate(spreadsheetId=GOOGLE_SHEET_ID, body=body).execute()
    return tab_or_gid

def _sheet_get_range_values(svc, title: str, a1: str) -> List[List[str]]:
    res=svc.spreadsheets().values().get(spreadsheetId=GOOGLE_SHEET_ID, range=f"'{title}'!{a1}").execute()
    return res.get("values", [])

def _sheet_update_range_values(svc, title: str, a1: str, values: List[List[str]]):
    svc.spreadsheets().values().update(
        spreadsheetId=GOOGLE_SHEET_ID, range=f"'{title}'!{a1}",
        valueInputOption="RAW", body={"values": values}).execute()

def sheet_load_notified() -> Dict[str, float]:
    svc=_sheet_service(); title=_resolve_sheet_title(svc, GOOGLE_SHEET_TAB)
    values=_sheet_get_range_values(svc, title, "A:C")
    start = 1 if values and values[0] and str(values[0][0]).upper() in ("KEY","RACEID","RID","ID") else 0
    d={}
    for row in values[start:]:
        if not row or len(row)<2: continue
        key=str(row[0]).strip()
        try: d[key]=float(row[1])
        except: pass
    return d

def sheet_upsert_notified(key: str, ts: float, note: str = "") -> None:
    svc=_sheet_service(); title=_resolve_sheet_title(svc, GOOGLE_SHEET_TAB)
    values=_sheet_get_range_values(svc, title, "A:C")
    header=["KEY","TS_EPOCH","NOTE"]
    if not values:
        _sheet_update_range_values(svc, title, "A:C", [header, [key, ts, note]]); return
    start_row=1 if values and values[0] and values[0][0] in header else 0
    found=None
    for i,row in enumerate(values[start_row:], start=start_row):
        if row and str(row[0]).strip()==key: found=i; break
    if found is None: values.append([key, ts, note])
    else: values[found]=[key, ts, note]
    _sheet_update_range_values(svc, title, "A:C", values)

# ========= é€ä¿¡å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ =========
def load_user_ids_from_simple_col() -> List[str]:
    svc=_sheet_service(); title=USERS_SHEET_NAME; col=USERS_USERID_COL.upper()
    values=_sheet_get_range_values(svc, title, f"{col}:{col}")
    user_ids=[]
    for i,row in enumerate(values):
        v=(row[0].strip() if row and row[0] is not None else "")
        if not v: continue
        low=v.replace(" ","").lower()
        if i==0 and ("userid" in low or "user id" in low or "line" in low): continue
        if not v.startswith("U"): continue
        if v not in user_ids: user_ids.append(v)
    logging.info("[INFO] usersã‚·ãƒ¼ãƒˆèª­è¾¼: %dä»¶ from tab=%s", len(user_ids), title)
    return user_ids

# ========= HTMLãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =========
def _extract_raceids_from_soup(soup: BeautifulSoup) -> List[str]:
    rids=[]
    for a in soup.find_all("a", href=True):
        m=RACEID_RE.search(a["href"])
        if m:
            rid=m.group(1)
            if not PLACEHOLDER.search(rid): rids.append(rid)
    return sorted(set(rids))

def _rid_date_parts(rid: str) -> Tuple[int,int,int]:
    return int(rid[0:4]), int(rid[4:6]), int(rid[6:8])

def _norm_hhmm_from_text(text: str) -> Optional[Tuple[int,int,str]]:
    if not text: return None
    s=str(text)
    for pat, tag in zip(TIME_PATS, ("half","full","kanji")):
        m=pat.search(s)
        if m:
            hh=int(m.group(1)); mm=int(m.group(2))
            if 0<=hh<=23 and 0<=mm<=59: return hh,mm,tag
    return None

def _make_dt_from_hhmm(rid: str, hh: int, mm: int) -> Optional[datetime]:
    try:
        y,mon,d=_rid_date_parts(rid)
        return datetime(y,mon,d,hh,mm,tzinfo=JST)
    except: return None

def _find_time_nearby(el: Tag) -> Tuple[Optional[str], str]:
    t=el.find("time")
    if t:
        for attr in ("datetime","data-time","title","aria-label"):
            v=t.get(attr)
            if v:
                got=_norm_hhmm_from_text(v)
                if got: hh,mm,why=got; return f"{hh:02d}:{mm:02d}", f"time@{attr}/{why}"
        got=_norm_hhmm_from_text(t.get_text(" ", strip=True))
        if got: hh,mm,why=got; return f"{hh:02d}:{mm:02d}", f"time@text/{why}"
    for node in el.find_all(True, recursive=True):
        for attr in ("data-starttime","data-start-time","data-time","title","aria-label"):
            v=node.get(attr)
            if not v: continue
            got=_norm_hhmm_from_text(v)
            if got: hh,mm,why=got; return f"{hh:02d}:{mm:02d}", f"data:{attr}/{why}"
    for sel in [".startTime",".cellStartTime",".raceTime",".time",".start-time"]:
        node=el.select_one(sel)
        if node:
            got=_norm_hhmm_from_text(node.get_text(" ", strip=True))
            if got: hh,mm,why=got; return f"{hh:02d}:{mm:02d}", f"sel:{sel}/{why}"
    got=_norm_hhmm_from_text(el.get_text(" ", strip=True))
    if got: hh,mm,why=got; return f"{hh:02d}:{mm:02d}", f"row:text/{why}"
    return None, "-"

# ========= ç™ºèµ°æ™‚åˆ»ï¼ˆä¸€è¦§ãƒšãƒ¼ã‚¸ï¼‰è§£æ =========
def parse_post_times_from_table_like(root: Tag) -> Dict[str, datetime]:
    post_map={}
    # ãƒ†ãƒ¼ãƒ–ãƒ«
    for table in root.find_all("table"):
        thead=table.find("thead")
        if thead:
            head_text="".join(thead.stripped_strings)
            if not any(k in head_text for k in ("ç™ºèµ°","ç™ºèµ°æ™‚åˆ»","ãƒ¬ãƒ¼ã‚¹")): continue
        body=table.find("tbody") or table
        for tr in body.find_all("tr"):
            rid=None; link=tr.find("a", href=True)
            if link:
                m=RACEID_RE.search(link["href"]); 
                if m: rid=m.group(1)
            if not rid or PLACEHOLDER.search(rid): continue
            hhmm,_=_find_time_nearby(tr)
            if not hhmm: continue
            hh,mm=map(int, hhmm.split(":"))
            dt=_make_dt_from_hhmm(rid, hh, mm)
            if dt: post_map[rid]=dt
    # ã‚«ãƒ¼ãƒ‰å‹
    for a in root.find_all("a", href=True):
        m=RACEID_RE.search(a["href"])
        if not m: continue
        rid=m.group(1)
        if PLACEHOLDER.search(rid) or rid in post_map: continue
        host=None; depth=0
        for parent in a.parents:
            if isinstance(parent, Tag) and parent.name in ("tr","li","div","section","article"):
                host=parent; break
            depth+=1
            if depth>=6: break
        host=host or a
        hhmm,_=_find_time_nearby(host)
        if not hhmm:
            sib_text=" ".join([x.get_text(" ", strip=True) for x in a.find_all_next(limit=4) if isinstance(x, Tag)])
            got=_norm_hhmm_from_text(sib_text)
            if got:
                hh,mm,_=got; hhmm=f"{hh:02d}:{mm:02d}"
        if not hhmm: continue
        hh,mm=map(int, hhmm.split(":"))
        dt=_make_dt_from_hhmm(rid, hh, mm)
        if dt: post_map[rid]=dt
    return post_map

def collect_post_time_map(ymd: str, ymd_next: str) -> Dict[str, datetime]:
    post_map={}
    def _merge_from(url: str):
        try:
            soup=BeautifulSoup(fetch(url), "lxml")
            post_map.update(parse_post_times_from_table_like(soup))
        except Exception as e:
            logging.warning(f"[WARN] ç™ºèµ°ä¸€è¦§èª­ã¿è¾¼ã¿å¤±æ•—: {e} ({url})")
    _merge_from(f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{ymd}0000000000")
    _merge_from(f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{ymd_next}0000000000")
    logging.info(f"[INFO] ç™ºèµ°æ™‚åˆ»å–å¾—: {len(post_map)}ä»¶")
    return post_map

# ========= ç· åˆ‡æ™‚åˆ»ï¼ˆæœ€å„ªå…ˆã§æŠ½å‡ºï¼‰ =========
def _extract_cutoff_hhmm_from_soup(soup: BeautifulSoup) -> Optional[str]:
    # ã‚»ãƒ¬ã‚¯ã‚¿å„ªå…ˆ
    for sel in ["time[data-type='cutoff']", ".cutoff time", ".deadline time", ".time.-deadline"]:
        t=soup.select_one(sel)
        if t:
            got=_norm_hhmm_from_text(t.get_text(" ", strip=True) or t.get("datetime",""))
            if got:
                hh,mm,_=got; return f"{hh:02d}:{mm:02d}"
    # ãƒ©ãƒ™ãƒ«è¿‘å‚
    for node in soup.find_all(string=CUTOFF_LABEL_PAT):
        container=getattr(node, "parent", None) or soup
        host=container
        for p in container.parents:
            if isinstance(p, Tag) and p.name in ("div","section","article","li"): host=p; break
        text=" ".join(host.get_text(" ", strip=True).split())
        got=_norm_hhmm_from_text(text)
        if got:
            hh,mm,_=got; return f"{hh:02d}:{mm:02d}"
    # å…¨æ–‡ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
    txt=" ".join(soup.stripped_strings)
    if CUTOFF_LABEL_PAT.search(txt):
        got=_norm_hhmm_from_text(txt)
        if got:
            hh,mm,_=got; return f"{hh:02d}:{mm:02d}"
    return None

def resolve_cutoff_dt(rid: str) -> Optional[Tuple[datetime, str]]:
    try:
        soup=BeautifulSoup(fetch(f"https://keiba.rakuten.co.jp/odds/tanfuku/RACEID/{rid}"), "lxml")
        hhmm=_extract_cutoff_hhmm_from_soup(soup)
        if hhmm:
            hh,mm=map(int, hhmm.split(":"))
            dt=_make_dt_from_hhmm(rid, hh, mm)
            if dt: return dt, "tanfuku"
    except Exception as e:
        logging.warning("[WARN] ç· åˆ‡æŠ½å‡º(tanfuku)å¤±æ•— rid=%s: %s", rid, e)
    try:
        soup=BeautifulSoup(fetch(f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{rid}"), "lxml")
        hhmm=_extract_cutoff_hhmm_from_soup(soup)
        if hhmm:
            hh,mm=map(int, hhmm.split(":"))
            dt=_make_dt_from_hhmm(rid, hh, mm)
            if dt: return dt, "list"
    except Exception as e:
        logging.warning("[WARN] ç· åˆ‡æŠ½å‡º(list)å¤±æ•— rid=%s: %s", rid, e)
    return None

# ========= ã‚ªãƒƒã‚ºè§£æï¼ˆå˜è¤‡ãƒšãƒ¼ã‚¸ï¼‰ =========
def _clean(s: str) -> str: return re.sub(r"\s+","", s or "")
def _as_float(text: str) -> Optional[float]:
    if not text: return None
    t=text.replace(",","").strip()
    if "%" in t or "-" in t or "ï½" in t or "~" in t: return None
    m=re.search(r"\d+(?:\.\d+)?", t); return float(m.group(0)) if m else None
def _as_int(text: str) -> Optional[int]:
    if not text: return None
    m=re.search(r"\d+", text); return int(m.group(0)) if m else None

def _find_popular_odds_table(soup: BeautifulSoup) -> Tuple[Optional[BeautifulSoup], Dict[str,int]]:
    for table in soup.find_all("table"):
        thead=table.find("thead"); 
        if not thead: continue
        headers=[_clean(th.get_text()) for th in thead.find_all(["th","td"])]
        if not headers: continue
        pop_idx=win_idx=num_idx=jockey_idx=None
        for i,h in enumerate(headers):
            if h in ("äººæ°—","é †ä½") or ("äººæ°—" in h and "é †" not in h): pop_idx=i; break
        win_c=[]
        for i,h in enumerate(headers):
            if ("è¤‡" in h) or ("ç‡" in h) or ("%" in h): continue
            if   h=="å˜å‹": win_c.append((0,i))
            elif "å˜å‹" in h: win_c.append((1,i))
            elif "ã‚ªãƒƒã‚º" in h: win_c.append((2,i))
        win_idx=sorted(win_c,key=lambda x:x[0])[0][1] if win_c else None
        for i,h in enumerate(headers):
            if "é¦¬ç•ª" in h: num_idx=i; break
        if num_idx is None:
            for i,h in enumerate(headers):
                if ("é¦¬" in h) and ("é¦¬å" not in h) and (i!=pop_idx): num_idx=i; break
        for i,h in enumerate(headers):
            if any(k in h for k in ("é¨æ‰‹","é¨æ‰‹å")): jockey_idx=i; break
        if pop_idx is None or win_idx is None: continue
        body=table.find("tbody") or table
        rows=body.find_all("tr")
        seq_ok,last=0,0
        for tr in rows[:6]:
            tds=tr.find_all(["td","th"])
            if len(tds)<=max(pop_idx,win_idx): continue
            s=tds[pop_idx].get_text(strip=True)
            if not s.isdigit(): break
            v=int(s)
            if v<=last: break
            last=v; seq_ok+=1
        if seq_ok>=2:
            return table, {"pop":pop_idx,"win":win_idx,"num":num_idx if num_idx is not None else -1,"jockey":jockey_idx if jockey_idx is not None else -1}
    return None, {}

def parse_odds_table(soup: BeautifulSoup) -> Tuple[List[Dict[str,float]], Optional[str], Optional[str]]:
    venue_race=(soup.find("h1").get_text(strip=True) if soup.find("h1") else None)
    nowtime=soup.select_one(".withUpdate .nowTime") or soup.select_one(".nowTime")
    now_label=nowtime.get_text(strip=True) if nowtime else None
    table, idx=_find_popular_odds_table(soup)
    if not table: return [], venue_race, now_label
    pop_idx=idx["pop"]; win_idx=idx["win"]; num_idx=idx.get("num",-1); jockey_idx=idx.get("jockey",-1)
    horses=[]; body=table.find("tbody") or table
    for tr in body.find_all("tr"):
        tds=tr.find_all(["td","th"])
        if len(tds)<=max(pop_idx,win_idx): continue
        pop_txt=tds[pop_idx].get_text(strip=True)
        if not pop_txt.isdigit(): continue
        pop=int(pop_txt)
        if not (1<=pop<=30): continue
        odds=_as_float(tds[win_idx].get_text(" ", strip=True))
        if odds is None: continue
        num=None
        if 0<=num_idx<len(tds): num=_as_int(tds[num_idx].get_text(" ", strip=True))
        jockey=None
        if 0<=jockey_idx<len(tds):
            jt=tds[jockey_idx].get_text(" ", strip=True)
            jraw=re.split(r"[ï¼ˆ( ]", jt)[0].strip() if jt else None
            jclean=_clean_jockey_name(jraw) if jraw else None
            jockey=jclean if jclean else None
        rec={"pop":pop,"odds":float(odds)}
        if num is not None: rec["num"]=num
        if jockey: rec["jockey"]=jockey
        horses.append(rec)
    # äººæ°—é‡è¤‡ã®æ’é™¤
    uniq={}
    for h in sorted(horses, key=lambda x:x["pop"]): uniq[h["pop"]]=h
    horses=[uniq[k] for k in sorted(uniq.keys())]
    return horses, venue_race, now_label

# === å‡ºé¦¬è¡¨ã‹ã‚‰ã®é¨æ‰‹è£œå®Œï¼ˆï¼‹è£œå®Œå¾Œã®å†æ­£è¦åŒ–ï¼‰ ===
def fetch_jockey_map_from_card(race_id: str) -> Dict[int, str]:
    urls=[f"https://keiba.rakuten.co.jp/race_card/RACEID/{race_id}",
          f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{race_id}"]
    result={}
    for url in urls:
        try: soup=BeautifulSoup(fetch(url),"lxml")
        except Exception: continue
        for table in soup.find_all("table"):
            thead=table.find("thead")
            if not thead: continue
            headers=[_clean(th.get_text()) for th in thead.find_all(["th","td"])]
            if not headers: continue
            num_idx=next((i for i,h in enumerate(headers) if "é¦¬ç•ª" in h), -1)
            jockey_idx=next((i for i,h in enumerate(headers) if any(k in h for k in ("é¨æ‰‹","é¨æ‰‹å"))), -1)
            if num_idx<0 or jockey_idx<0: continue
            body=table.find("tbody") or table
            for tr in body.find_all("tr"):
                tds=tr.find_all(["td","th"])
                if len(tds)<=max(num_idx, jockey_idx): continue
                num=_as_int(tds[num_idx].get_text(" ", strip=True))
                jtx=tds[jockey_idx].get_text(" ", strip=True)
                if num is None or not jtx: continue
                name=_clean_jockey_name(re.split(r"[ï¼ˆ(]", jtx)[0])
                if name: result[num]=name
            if result: return result
    return result

def _enrich_horses_with_jockeys(horses: List[Dict[str,float]], race_id: str) -> None:
    need=any((h.get("jockey") is None) and isinstance(h.get("num"), int) for h in horses)
    num2jockey=fetch_jockey_map_from_card(race_id) if need else {}
    for h in horses:
        if (not h.get("jockey")) and isinstance(h.get("num"), int):
            name=num2jockey.get(h["num"])
            if name: h["jockey"]=_clean_jockey_name(name)
        if h.get("jockey"):
            h["jockey"]=_clean_jockey_name(h["jockey"])  # å†æ­£è¦åŒ–

def _debug_jockey_match(horses: List[Dict[str,float]]):
    for h in sorted(horses, key=lambda x:int(x.get("pop",999)))[:5]:
        raw=h.get("jockey") or ""
        norm=_normalize_name(_clean_jockey_name(raw))
        r=jockey_rank_letter_by_name(raw)
        logging.debug("[RANKDBG] pop=%s uma=%s jockey_raw=%s norm=%s rank=%s",
                      h.get("pop"), h.get("num"), raw, norm, r)

def check_tanfuku_page(race_id: str) -> Optional[Dict]:
    url=f"https://keiba.rakuten.co.jp/odds/tanfuku/RACEID/{race_id}"
    html=fetch(url); soup=BeautifulSoup(html,"lxml")
    horses, venue_race, now_label = parse_odds_table(soup)
    if not horses: return None
    if not venue_race: venue_race="åœ°æ–¹ç«¶é¦¬"
    _enrich_horses_with_jockeys(horses, race_id)
    _debug_jockey_match(horses)
    return {"race_id": race_id, "url": url, "horses": horses, "venue_race": venue_race, "now": now_label or ""}

# ========= ç™ºèµ°æ™‚åˆ»ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ =========
def fallback_post_time_for_rid(rid: str) -> Optional[Tuple[datetime, str, str]]:
    def _from_list_page():
        url=f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{rid}"
        soup=BeautifulSoup(fetch(url),"lxml")
        a=soup.find("a", href=re.compile(rf"/RACEID/{rid}"))
        if not a: return None
        host=None
        for parent in a.parents:
            if isinstance(parent, Tag) and parent.name in ("tr","li","div","section","article"):
                host=parent; break
        host=host or a
        hhmm,reason=_find_time_nearby(host)
        if not hhmm:
            sibs=[n for n in host.find_all_next(limit=6) if isinstance(n, Tag)]
            text=" ".join([n.get_text(" ", strip=True) for n in sibs])
            if POST_LABEL_PAT.search(text):
                got=_norm_hhmm_from_text(text)
                if got:
                    hh,mm,why=got; hhmm,reason=f"{hh:02d}:{mm:02d}", f"sibling:label-first/{why}"
            else:
                if not IGNORE_NEAR_PAT.search(text):
                    got=_norm_hhmm_from_text(text)
                    if got:
                        hh,mm,why=got; hhmm,reason=f"{hh:02d}:{mm:02d}", f"sibling:text/{why}"
        if not hhmm: return None
        hh,mm=map(int, hhmm.split(":")); dt=_make_dt_from_hhmm(rid, hh, mm)
        return (dt, f"list-anchor/{reason}", url) if dt else None

    def _from_tanfuku_page():
        url=f"https://keiba.rakuten.co.jp/odds/tanfuku/RACEID/{rid}"
        soup=BeautifulSoup(fetch(url),"lxml")
        for key in ("ç™ºèµ°","ç™ºèµ°æ™‚åˆ»","ç™ºèµ°äºˆå®š","ç™ºé€","å‡ºèµ°"):
            for node in soup.find_all(string=re.compile(key)):
                el=getattr(node,"parent",None) or soup
                container=el
                for parent in el.parents:
                    if isinstance(parent, Tag) and parent.name in ("div","section","article","li"):
                        container=parent; break
                near=" ".join((container.get_text(" ", strip=True) or "").split())
                if IGNORE_NEAR_PAT.search(near) and not POST_LABEL_PAT.search(near):
                    continue
                got=_norm_hhmm_from_text(near)
                if got:
                    hh,mm,why=got; dt=_make_dt_from_hhmm(rid, hh, mm)
                    if dt: return dt, f"tanfuku-label/{key}/{why}", url
        return None

    try:
        got=_from_list_page()
        if got: return got
    except Exception as e:
        logging.warning("[WARN] fallback(list)å¤±æ•— rid=%s: %s", e)
    try:
        got=_from_tanfuku_page()
        if got: return got
    except Exception as e:
        logging.warning("[WARN] fallback(tanfuku)å¤±æ•— rid=%s: %s", e)
    return None

# ========= RACEID åˆ—æŒ™ =========
def list_raceids_today_ticket(ymd: str) -> List[str]:
    url=f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{ymd}0000000000"
    soup=BeautifulSoup(fetch(url),"lxml")
    ids=_extract_raceids_from_soup(soup)
    logging.info(f"[INFO] Rakuten#1 æœ¬æ—¥ã®ç™ºå£²æƒ…å ±: {len(ids)}ä»¶")
    return ids

def list_raceids_from_card_lists(ymd: str, ymd_next: str) -> List[str]:
    urls=[f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{ymd}0000000000",
          f"https://keiba.rakuten.co.jp/race_card/list/RACEID/{ymd_next}0000000000"]
    rids=[]
    for u in urls:
        try:
            soup=BeautifulSoup(fetch(u),"lxml")
            rids.extend(_extract_raceids_from_soup(soup))
        except Exception as e:
            logging.warning(f"[WARN] å‡ºé¦¬è¡¨ä¸€è¦§ã‚¹ã‚­ãƒ£ãƒ³å¤±æ•—: {e} ({u})")
    rids=sorted(set(rids))
    logging.info(f"[INFO] Rakuten#2 å‡ºé¦¬è¡¨ä¸€è¦§: {len(rids)}ä»¶")
    return rids

# ========= çª“åˆ¤å®š =========
def is_within_window(target_dt: datetime, now: datetime, before_min:int=WINDOW_BEFORE_MIN,
                     after_min:int=WINDOW_AFTER_MIN, grace_sec:int=GRACE_SECONDS) -> bool:
    win_start = target_dt - timedelta(minutes=before_min, seconds=grace_sec)
    win_end   = target_dt + timedelta(minutes=after_min,  seconds=grace_sec)
    return win_start <= now <= win_end

# ========= LINE =========
def push_line_text(user_id: str, token: str, text: str, timeout=8, retries=1):
    headers={"Authorization": f"Bearer {token}", "Content-Type": "application/json"}
    payload={"to": user_id, "messages":[{"type":"text","text": text}]}
    for attempt in range(retries+1):
        try:
            resp=requests.post(LINE_PUSH_URL, headers=headers, json=payload, timeout=timeout)
            if resp.status_code==200: return True, 200, resp.text
            if resp.status_code==429 and attempt<retries:
                wait=int(resp.headers.get("Retry-After","1")); time.sleep(max(wait,1)); continue
            return False, resp.status_code, resp.text
        except requests.RequestException as e:
            if attempt<retries: time.sleep(2); continue
            return False, None, str(e)

def notify_strategy_hit_to_many(message_text: str, targets: List[str]):
    if not NOTIFY_ENABLED: logging.info("[INFO] NOTIFY_ENABLED=0"); return False, None
    if DRY_RUN: logging.info("[DRY_RUN] é€šçŸ¥:\n%s", message_text); return False, None
    if not LINE_ACCESS_TOKEN: logging.error("[ERROR] LINE_ACCESS_TOKEN ä¸è¶³"); return False, None
    if not targets: logging.error("[ERROR] é€ä¿¡å…ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ãªã—"); return False, None
    all_ok=True; last=None
    for uid in targets:
        ok, status, _ = push_line_text(uid, LINE_ACCESS_TOKEN, message_text)
        last=status
        if not ok: all_ok=False
        time.sleep(0.2)
    return all_ok, last

# ========= è¡¨ç¤ºç”¨ãƒãƒƒãƒ— =========
def _map_pop_info(horses: List[Dict[str,float]]) -> Dict[int, Dict[str, Optional[float]]]:
    m={}
    for h in horses:
        try:
            p=int(h.get("pop"))
            num=h.get("num") if isinstance(h.get("num"), int) else None
            o=float(h.get("odds")) if h.get("odds") is not None else None
            j=h.get("jockey") or None
            m[p]={"umaban":num,"odds":o,"jockey":j}
        except: pass
    return m

def _map_umaban_info(horses: List[Dict[str,float]]) -> Dict[int, Dict[str, Optional[float]]]:
    out={}
    for h in horses:
        try:
            p=int(h.get("pop"))
            n=int(h.get("num")) if h.get("num") is not None else None
            o=float(h.get("odds")) if h.get("odds") is not None else None
            j=h.get("jockey") or None
            if n is not None: out[n]={"pop":p,"odds":o,"jockey":j}
        except: pass
    return out

# === äººæ°—å„ªå…ˆã§è¡¨ç¤ºï¼ˆæˆ¦ç•¥1/2/4ï¼‰ ===
def _format_single_leg_prefer_pop(n:int, pop2:Dict[int,Dict], uma2:Dict[int,Dict]) -> Optional[str]:
    # ã¾ãšã€äººæ°—ã€ã¨ã—ã¦è§£é‡ˆ â†’ é¦¬ç•ªã¸
    pinf = pop2.get(n)
    if pinf and (pinf.get("umaban") is not None) and (pinf.get("odds") is not None):
        uma=int(pinf["umaban"]); odds=float(pinf["odds"]); jk=pinf.get("jockey")
        rank=jockey_rank_letter_by_name(jk) if jk else "â€”"
        return f"{uma}ç•ªï¼ˆ{n}äººæ°—ï¼{odds:.1f}å€ï¼{rank}ï¼‰"
    # ãƒ€ãƒ¡ãªã‚‰ã€é¦¬ç•ªã€ã¨ã—ã¦è§£é‡ˆ
    uinf = uma2.get(n)
    if uinf and (uinf.get("pop") is not None) and (uinf.get("odds") is not None):
        pop=int(uinf["pop"]); odds=float(uinf["odds"]); jk=uinf.get("jockey")
        rank=jockey_rank_letter_by_name(jk) if jk else "â€”"
        return f"{n}ç•ªï¼ˆ{pop}äººæ°—ï¼{odds:.1f}å€ï¼{rank}ï¼‰"
    return None

def _format_bet_display_line(ticket: str, horses: List[Dict[str,float]], prefer:str="pop") -> str:
    nums=[int(x) for x in re.findall(r"\d+", str(ticket))]
    if not nums: return ticket
    pop2=_map_pop_info(horses); uma2=_map_umaban_info(horses)
    parts=[]
    for n in nums:
        label = (_format_single_leg_prefer_pop(n, pop2, uma2)
                 if prefer=="pop" else
                 (lambda x: None)(n))
        parts.append(label if label else str(n))
    return " - ".join(parts)

def _format_bets_umaban_odds_rank(bets: List[str], horses: List[Dict[str,float]], prefer:str="pop") -> List[str]:
    return [_format_bet_display_line(b, horses, prefer=prefer) for b in bets]

# ========= é€šçŸ¥æœ¬æ–‡ï¼ˆâ‘ â‘¡â‘£ å…±é€šï¼‰ =========
_CIRCLED="â‘ â‘¡â‘¢â‘£â‘¤â‘¥â‘¦â‘§â‘¨"
def _circled(n:int)->str: return _CIRCLED[n-1] if 1<=n<=9 else f"{n}."
def _extract_hhmm_label(s:str)->Optional[str]:
    got=_norm_hhmm_from_text(s)
    if not got: return None
    hh,mm,_=got; return f"{hh:02d}:{mm:02d}"
def _infer_pattern_no(strategy_text: str) -> int:
    if not strategy_text: return 0
    m=re.match(r"\s*([â‘ -â‘¨])", strategy_text)
    if m: return _CIRCLED.index(m.group(1))+1
    m=re.match(r"\s*(\d+)", strategy_text)
    if m:
        try: return int(m.group(1))
        except: return 0
    return 0
def _strip_pattern_prefix(strategy_text: str) -> str:
    s=re.sub(r"^\s*[â‘ -â‘¨]\s*", "", strategy_text or "")
    s=re.sub(r"^\s*\d+\s*", "", s); return s.strip()
def _split_venue_race(venue_race: str) -> Tuple[str,str]:
    if not venue_race: return "åœ°æ–¹ç«¶é¦¬",""
    m=re.search(r"^\s*([^\s\d]+)\s*(\d{1,2}R)\b", venue_race)
    if m:
        venue=m.group(1); race=m.group(2)
        venue_disp = f"{venue}ç«¶é¦¬å ´" if "ç«¶é¦¬" not in venue else venue
        return venue_disp, race
    return venue_race, ""

def build_line_notification(pattern_no:int, venue:str, race_no:str, time_label:str, time_hm:str,
                            condition_text:str, raw_bets:List[str], odds_timestamp_hm:Optional[str],
                            odds_url:str, horses:List[Dict[str,float]]) -> str:
    title=f"ã€æˆ¦ç•¥{pattern_no if pattern_no>0 else ''}è©²å½“ãƒ¬ãƒ¼ã‚¹ç™ºè¦‹ğŸ’¡ã€‘".replace("æˆ¦ç•¥è©²å½“","æˆ¦ç•¥è©²å½“")
    lines=[title, f"â– ãƒ¬ãƒ¼ã‚¹ï¼š{venue} {race_no}ï¼ˆ{time_label} {time_hm}ï¼‰".strip()]
    if condition_text: lines.append(f"â– æ¡ä»¶ï¼š{condition_text}")
    lines+=["", "â– è²·ã„ç›®ï¼ˆé¦¬ç•ªãƒ»ã‚ªãƒƒã‚ºãƒ»é¨æ‰‹ãƒ©ãƒ³ã‚¯ï¼‰ï¼š"]
    pretty=_format_bets_umaban_odds_rank(raw_bets, horses, prefer="pop")
    for i,bet in enumerate(pretty,1): lines.append(f"{_circled(i)} {bet}")
    if odds_timestamp_hm: lines+=["", f"ğŸ“… ã‚ªãƒƒã‚ºæ™‚ç‚¹: {odds_timestamp_hm}"]
    lines+=["ğŸ”— ã‚ªãƒƒã‚ºè©³ç´°:", odds_url, ""]
    lines+=[
        "â€»ã‚ªãƒƒã‚ºã¯ç· åˆ‡ç›´å‰ã¾ã§å¤‰å‹•ã—ã¾ã™ã€‚",
        "â€»é¨æ‰‹ãƒ©ãƒ³ã‚¯ã¯2024å¹´ã®åœ°æ–¹ç«¶é¦¬ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¸ãƒ§ãƒƒã‚­ãƒ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«åŸºã¥ãã€A=1ã€œ70ä½ / B=71ã€œ200ä½ / C=ãã®ä»–ã€‚",
        "â€»é¦¬åˆ¸è³¼å…¥ã¯ä½™è£•è³‡é‡‘ã§ã€‚çš„ä¸­ã¯ä¿è¨¼ã•ã‚Œã¾ã›ã‚“ã€‚"
    ]
    return "\n".join(lines)

# ========= â‘¢å°‚ç”¨ =========
def build_line_notification_strategy3(strategy:Dict, venue:str, race_no:str, time_label:str, time_hm:str,
                                      odds_timestamp_hm:Optional[str], odds_url:str,
                                      horses:List[Dict[str,float]]) -> str:
    pop2=_map_pop_info(horses)
    axis=strategy.get("axis") or {}
    axis_pop=axis.get("pop") or 1
    axis_rank=jockey_rank_letter_by_name((pop2.get(axis_pop) or {}).get("jockey"))
    axis_uma=(pop2.get(axis_pop) or {}).get("umaban")
    axis_odds=(pop2.get(axis_pop) or {}).get("odds")
    axis_label = f"{int(axis_uma)}ç•ªï¼ˆ{axis_pop}äººæ°—ï¼{float(axis_odds):.1f}å€ï¼{axis_rank}ï¼‰" if axis_uma and axis_odds else f"{axis_pop}äººæ°—ï¼ˆâ€”ï¼â€”ï¼{axis_rank}ï¼‰"

    cands=strategy.get("candidates")
    if not cands:
        cands=[]
        for h in sorted(horses, key=lambda x:int(x.get("pop",999))):
            try:
                p=int(h.get("pop")); o=float(h.get("odds"))
                if p==1: continue
                if 10.0<=o<=20.0:
                    cands.append({"pop":p,"jockey":h.get("jockey")})
                    if len(cands)>=4: break
            except: pass

    def _cand_label(c):
        p=c.get("pop"); pinf=pop2.get(p) or {}
        uma=pinf.get("umaban"); odds=pinf.get("odds"); rank=jockey_rank_letter_by_name(pinf.get("jockey"))
        if uma and odds is not None:
            return f"{int(uma)}ç•ªï¼ˆ{p}äººæ°—ï¼{float(odds):.1f}å€ï¼{rank}ï¼‰"
        return f"{p}äººæ°—ï¼ˆâ€”ï¼â€”ï¼{rank}ï¼‰"
    cand_labels=[_cand_label(c) for c in sorted(cands, key=lambda x:x.get("pop",999))]

    tickets=strategy.get("tickets") or []
    pretty=_format_bets_umaban_odds_rank(tickets, horses, prefer="pop")

    title="ã€æˆ¦ç•¥â‘¢è©²å½“ãƒ¬ãƒ¼ã‚¹ç™ºè¦‹ğŸ’¡ã€‘"
    cond_line="1ç•ªäººæ°— â‰¤2.0ã€2ç•ªäººæ°— â‰¥10.0ã€ç›¸æ‰‹ï¼å˜å‹10ã€œ20å€ï¼ˆæœ€å¤§4é ­ï¼‰"
    n=len(cand_labels); pts=n*(n-1) if n>=2 else 0

    lines=[title,
           f"â– ãƒ¬ãƒ¼ã‚¹ï¼š{venue} {race_no}ï¼ˆ{time_label} {time_hm}ï¼‰",
           f"â– æ¡ä»¶ï¼š{cond_line}",
           f"â– è»¸ï¼š{axis_label}",
           f"â– ç›¸æ‰‹å€™è£œï¼š{', '.join(cand_labels) if cand_labels else 'â€”'}",
           f"â– è²·ã„ç›®ï¼ˆ3é€£å˜ãƒ»1ç€å›ºå®šï¼‰ï¼š{', '.join(pretty) if pretty else 'â€”'}",
           f"  â†’ å€™è£œ {n}é ­ï¼åˆè¨ˆ {pts}ç‚¹"
    ]
    if odds_timestamp_hm: lines += [f"\nğŸ“… ã‚ªãƒƒã‚ºæ™‚ç‚¹: {odds_timestamp_hm}"]
    lines += ["ğŸ”— ã‚ªãƒƒã‚ºè©³ç´°:", odds_url, ""]
    lines += [
        "â€»ã‚ªãƒƒã‚ºã¯ç· åˆ‡ç›´å‰ã¾ã§å¤‰å‹•ã—ã¾ã™ã€‚",
        "â€»é¨æ‰‹ãƒ©ãƒ³ã‚¯ã¯2024å¹´ã®åœ°æ–¹ç«¶é¦¬ãƒªãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚¸ãƒ§ãƒƒã‚­ãƒ¼ãƒ©ãƒ³ã‚­ãƒ³ã‚°ã«åŸºã¥ãã€A=1ã€œ70ä½ / B=71ã€œ200ä½ / C=ãã®ä»–ã€‚",
        "â€»é¦¬åˆ¸è³¼å…¥ã¯ä½™è£•è³‡é‡‘ã§ã€‚çš„ä¸­ã¯ä¿è¨¼ã•ã‚Œã¾ã›ã‚“ã€‚"
    ]
    return "\n".join(lines)

# ========= ãƒ™ãƒƒãƒˆè¨˜éŒ² =========
def _bets_sheet_header() -> List[str]:
    return ["date","race_id","venue","race_no","strategy","bet_kind","tickets_umaban_csv","points","unit_stake","total_stake"]

def sheet_append_bet_record(date_ymd:str, race_id:str, venue:str, race_no:str,
                            strategy_no:int, bet_kind:str, tickets_umaban:List[str]):
    svc=_sheet_service(); title=_resolve_sheet_title(svc, BETS_SHEET_TAB)
    values=_sheet_get_range_values(svc, title, "A:J")
    if not values: values=[_bets_sheet_header()]
    points=len(tickets_umaban); unit=UNIT_STAKE_YEN; total=points*unit
    values.append([date_ymd, race_id, venue, race_no, str(strategy_no), bet_kind, ",".join(tickets_umaban), str(points), str(unit), str(total)])
    _sheet_update_range_values(svc, title, "A:J", values)

# ========= æ‰•æˆ»å–å¾—ï¼†æ—¥æ¬¡ã‚µãƒãƒª =========
_PAYOUT_KIND_KEYS = ["å˜å‹","è¤‡å‹","æ é€£","é¦¬é€£","ãƒ¯ã‚¤ãƒ‰","é¦¬å˜","ä¸‰é€£è¤‡","ä¸‰é€£å˜"]

def fetch_payoff_map(race_id:str) -> Dict[str, List[Tuple[str,int]]]:
    url=f"https://keiba.rakuten.co.jp/race/payoff/RACEID/{race_id}"
    html=fetch(url); soup=BeautifulSoup(html,"lxml")
    result={}
    for kind in _PAYOUT_KIND_KEYS:
        blocks=soup.find_all(string=re.compile(kind))
        items=[]
        for b in blocks:
            box=getattr(b,"parent",None) or soup
            text=" ".join((box.get_text(" ", strip=True) or "").split())
            for m in re.finditer(r"(\d+(?:-\d+){0,2})\s*([\d,]+)\s*å††", text):
                comb=m.group(1); pay=int(m.group(2).replace(",",""))
                items.append((comb, pay))
        if items: result[kind]=items
    return result

def _normalize_ticket_for_kind(ticket:str, kind:str) -> str:
    parts=[int(x) for x in ticket.split("-") if x.strip().isdigit()]
    if kind in ("é¦¬é€£","ä¸‰é€£è¤‡"): parts=sorted(parts)
    return "-".join(str(x) for x in parts)

def _summary_key_for_today() -> str:
    return f"summary:{now_jst():%Y%m%d}"

def _is_time_reached(now: datetime, hhmm: str) -> bool:
    """æŒ‡å®šhh:mmï¼ˆJSTï¼‰ã«åˆ°é”æ¸ˆã¿ã‹ã€‚>= ã§åˆ¤å®šï¼ˆã‚¸ãƒ§ãƒ–ãŒé…ã‚Œã¦ã‚‚1å›ã¯å‹•ãï¼‰"""
    try: hh,mm=map(int, hhmm.split(":"))
    except Exception: return False
    target=now.replace(hour=hh, minute=mm, second=0, microsecond=0)
    return now >= target

def summarize_today_and_notify(targets: List[str]):
    """å½“æ—¥betsã‹ã‚‰ã‚µãƒãƒªã‚’ä½œæˆã—é€šçŸ¥ã€‚æ‰•æˆ»ãƒšãƒ¼ã‚¸å–å¾—å¤±æ•—ã¯å€‹åˆ¥ã«æ¡ã‚Šã¤ã¶ã—ã¦ç¶™ç¶šã€‚"""
    logging.info("[SUMMARY] æœ¬æ—¥ã‚µãƒãƒªä½œæˆã‚’é–‹å§‹ã—ã¾ã™")

    svc=_sheet_service(); title=_resolve_sheet_title(svc, BETS_SHEET_TAB)
    values=_sheet_get_range_values(svc, title, "A:J")
    if not values or values==[_bets_sheet_header()]:
        if not ALWAYS_NOTIFY_DAILY_SUMMARY:
            logging.info("[SUMMARY] betsã‚·ãƒ¼ãƒˆã«å½“æ—¥ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆç„¡é€šçŸ¥ãƒ¢ãƒ¼ãƒ‰ï¼‰")
            return
        values=[_bets_sheet_header()]

    hdr=values[0]; rows=values[1:]
    today=now_jst().strftime("%Y%m%d")
    records=[r for r in rows if len(r)>=10 and r[0]==today]

    if not records and not ALWAYS_NOTIFY_DAILY_SUMMARY:
        logging.info("[SUMMARY] å½“æ—¥åˆ†ãªã—ï¼ˆç„¡é€šçŸ¥ãƒ¢ãƒ¼ãƒ‰ï¼‰")
        return

    per_strategy = { k:{"races":0,"hits":0,"bets":0,"stake":0,"return":0} for k in ("1","2","3","4") }
    seen_race_strategy:set[Tuple[str,str]] = set()

    for r in records:
        date_ymd, race_id, venue, race_no, strategy, bet_kind, t_csv, points, unit, total = r[:10]
        if (race_id, strategy) not in seen_race_strategy:
            seen_race_strategy.add((race_id, strategy))
            per_strategy[strategy]["races"] += 1
        tickets=[t for t in t_csv.split(",") if t]
        per_strategy[strategy]["bets"]  += len(tickets)
        per_strategy[strategy]["stake"] += int(total)

        # æ‰•æˆ»ãƒšãƒ¼ã‚¸å–å¾—ã¯ãƒ¬ãƒ¼ã‚¹ã”ã¨ã«ä¿è­·
        try:
            paymap=fetch_payoff_map(race_id)
        except Exception as e:
            logging.warning("[SUMMARY] æ‰•æˆ»å–å¾—å¤±æ•— rid=%s: %sï¼ˆã“ã®ãƒ¬ãƒ¼ã‚¹ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰", race_id, e)
            continue

        winners={ _normalize_ticket_for_kind(comb, bet_kind): pay for (comb, pay) in paymap.get(bet_kind, []) }
        for t in tickets:
            norm=_normalize_ticket_for_kind(t, bet_kind)
            if norm in winners:
                per_strategy[strategy]["hits"]   += 1
                per_strategy[strategy]["return"] += winners[norm]
        time.sleep(0.2)

    total_stake=sum(v["stake"] for v in per_strategy.values())
    total_return=sum(v["return"] for v in per_strategy.values())
    def pct(n,d): return f"{(100.0*n/d):.1f}%" if d>0 else "0.0%"

    lines=["ğŸ“Šã€æœ¬æ—¥ã®æ¤œè¨¼çµæœã€‘", f"æ—¥ä»˜ï¼š{today[:4]}/{today[4:6]}/{today[6:]}", ""]
    for k in ("1","2","3","4"):
        v=per_strategy[k]
        hit_rate=pct(v["hits"], max(v["bets"],1))
        roi=pct(v["return"], max(v["stake"],1))
        lines.append(f"æˆ¦ç•¥{k}ï¼šè©²å½“{v['races']}ãƒ¬ãƒ¼ã‚¹ / è³¼å…¥{v['bets']}ç‚¹ / çš„ä¸­{v['hits']}ç‚¹")
        lines.append(f"ã€€ã€€ã€€çš„ä¸­ç‡ {hit_rate} / å›åç‡ {roi}")
    lines.append("")
    lines.append(f"åˆè¨ˆï¼šæŠ•è³‡ {total_stake:,}å†† / æ‰•æˆ» {total_return:,}å†† / å›åç‡ {pct(total_return, max(total_stake,1))}")

    msg="\n".join(lines)
    ok, status = notify_strategy_hit_to_many(msg, targets)
    if ok:
        logging.info("[SUMMARY] ã‚µãƒãƒªé€šçŸ¥ã‚’é€ä¿¡ã—ã¾ã—ãŸï¼ˆHTTP %sï¼‰", status)
    else:
        logging.error("[SUMMARY] ã‚µãƒãƒªé€šçŸ¥ã®é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸï¼ˆHTTP %sï¼‰", status)

# ========= ç›£è¦–æœ¬ä½“ï¼ˆä¸€å›å®Ÿè¡Œï¼‰ =========
def _tickets_pop_to_umaban(bets: List[str], horses: List[Dict[str,float]]) -> List[str]:
    """betsï¼ˆäººæ°— or é¦¬ç•ªæ··åœ¨ï¼‰â†’ é¦¬ç•ªåˆ—ã¸å¤‰æ›ï¼ˆbetsã‚·ãƒ¼ãƒˆä¿å­˜ç”¨ï¼‰"""
    pop2=_map_pop_info(horses); out=[]
    for b in bets:
        nums=[int(x) for x in re.findall(r"\d+", str(b))]
        if not nums: out.append(b); continue
        res=[]; ok=True
        for n in nums:
            # é¦¬ç•ªã¨ã—ã¦æ—¢å­˜ãªã‚‰ãã®ã¾ã¾
            if any((isinstance(h.get("num"), int) and int(h.get("num"))==n) for h in horses):
                res.append(str(n)); continue
            # äººæ°—â†’é¦¬ç•ª
            u=(pop2.get(n) or {}).get("umaban")
            if u is None: ok=False; break
            res.append(str(u))
        out.append("-".join(res) if ok else b)
    return out

def main():
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    p=pathlib.Path(__file__).resolve()
    sha=hashlib.sha1(p.read_bytes()).hexdigest()[:12]
    logging.info(f"[BUILD] file={p} sha1={sha} v2025-08-15E")

    if KILL_SWITCH:
        logging.info("[INFO] KILL_SWITCH=True"); return

    # é€ä¿¡ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ
    try:
        targets=load_user_ids_from_simple_col()
        if not targets:
            fb=LINE_USER_IDS if LINE_USER_IDS else ([LINE_USER_ID] if LINE_USER_ID else [])
            targets=fb
    except Exception as e:
        logging.exception("[ERROR] usersã‚·ãƒ¼ãƒˆèª­è¾¼å¤±æ•—: %s", e)
        fb=LINE_USER_IDS if LINE_USER_IDS else ([LINE_USER_ID] if LINE_USER_ID else [])
        targets=fb
    logging.info("[INFO] é€ä¿¡å…ˆ=%d", len(targets))

    # ç¨¼åƒæ™‚é–“å†…ã§é€šå¸¸ç›£è¦–
    if within_operating_hours():
        try:
            notified=sheet_load_notified()
        except Exception as e:
            logging.exception("[ERROR] TTLãƒ­ãƒ¼ãƒ‰å¤±æ•—: %s", e)
            notified={}
        if DEBUG_RACEIDS:
            target_raceids=[rid for rid in DEBUG_RACEIDS if not PLACEHOLDER.search(rid)]
            post_time_map={}
        else:
            ymd=now_jst().strftime("%Y%m%d")
            ymd_next=(now_jst()+timedelta(days=1)).strftime("%Y%m%d")
            r1=list_raceids_today_ticket(ymd)
            r2=list_raceids_from_card_lists(ymd, ymd_next)
            target_raceids=sorted(set(r1)|set(r2))
            post_time_map=collect_post_time_map(ymd, ymd_next)
            target_raceids=[rid for rid in target_raceids if not PLACEHOLDER.search(rid)]

        hits=0; matches=0
        seen_in_this_run:set[str]=set()

        for rid in target_raceids:
            if rid in seen_in_this_run: continue
            now_ts=time.time()
            cd_ts=notified.get(f"{rid}:cd")
            if cd_ts and (now_ts-cd_ts)<NOTIFY_COOLDOWN_SEC: continue
            ts=notified.get(rid)
            if ts and (now_ts-ts)<NOTIFY_TTL_SEC: continue

            # ç™ºèµ°ï¼ˆåŸºæº–ï¼‰å–å¾—
            post_time=post_time_map.get(rid)
            if not post_time:
                got=fallback_post_time_for_rid(rid)
                if got: post_time, _, _ = got
                else: 
                    logging.info("[TRACE] time rid=%s result=SKIP reason=no_post_time", rid)
                    continue

            # ç· åˆ‡å–å¾—ï¼ˆæœ€å„ªå…ˆï¼‰
            cutoff_info=resolve_cutoff_dt(rid) if CUTOFF_OFFSET_MIN>0 else None
            if cutoff_info:
                cutoff_dt, cutoff_src = cutoff_info
                used_dt = cutoff_dt
                time_label = "ç· åˆ‡"
                src_label  = f"cutoff:{cutoff_src}"
            else:
                used_dt = post_time - timedelta(minutes=CUTOFF_OFFSET_MIN) if CUTOFF_OFFSET_MIN>0 else post_time
                time_label = "ç· åˆ‡" if CUTOFF_OFFSET_MIN>0 else "ç™ºèµ°"
                src_label  = "post-offset" if CUTOFF_OFFSET_MIN>0 else "post"

            now=now_jst()
            in_win=is_within_window(used_dt, now)
            logging.info("[TRACE] time rid=%s use=%s src=%s at=%s target=%s Î”sec=%.1f in_window=%s",
                         rid, time_label, src_label, now.strftime("%H:%M:%S"), used_dt.strftime("%H:%M"),
                         (used_dt-now).total_seconds(), in_win)
            if not in_win: continue

            meta=check_tanfuku_page(rid)
            if not meta:
                logging.info("[TRACE] odds rid=%s result=SKIP reason=no_table", rid)
                time.sleep(random.uniform(*SLEEP_BETWEEN)); continue

            horses=meta["horses"]
            if len(horses)<4:
                logging.info("[TRACE] odds rid=%s result=SKIP reason=too_few_horses len=%d", rid, len(horses))
                time.sleep(random.uniform(*SLEEP_BETWEEN)); continue

            # ã‚ªãƒƒã‚ºã‚¹ãƒŠãƒƒãƒ—
            top3=sorted(horses, key=lambda x:int(x.get("pop",999)))[:3]
            snapshot=[(int(h.get("pop",0)), float(h.get("odds",0.0))) for h in top3 if "pop" in h and "odds" in h]
            logging.info("[TRACE] odds_top3 rid=%s %s", rid, snapshot)

            hits+=1
            strategy=eval_strategy(horses, logger=logging)
            if not strategy:
                logging.info("[TRACE] judge rid=%s result=FAIL reason=no_strategy_match", rid)
                time.sleep(random.uniform(*SLEEP_BETWEEN)); continue
            matches+=1
            logging.info("[TRACE] judge rid=%s result=PASS strat=%s", rid, strategy.get("strategy",""))

            strategy_text=strategy.get("strategy","")
            pattern_no=_infer_pattern_no(strategy_text)
            condition_text=_strip_pattern_prefix(strategy_text) or strategy_text

            venue_disp, race_no=_split_venue_race(meta.get("venue_race",""))
            display_dt=used_dt
            time_hm=display_dt.strftime("%H:%M")
            odds_hm=_extract_hhmm_label(meta.get("now",""))

            raw_tickets=strategy.get("tickets", [])
            if isinstance(raw_tickets, str):
                raw_tickets=[s.strip() for s in raw_tickets.split(",") if s.strip()]

            # é€šçŸ¥æœ¬æ–‡
            if str(strategy_text).startswith("â‘¢"):
                message=build_line_notification_strategy3(strategy, venue_disp, race_no, time_label, time_hm, odds_hm, meta["url"], horses)
                tickets_umaban = strategy.get("tickets", [])  # â‘¢ã¯umabanç”Ÿæˆã®ã“ã¨ãŒå¤šã„
                bet_kind = STRATEGY_BET_KIND.get("3", "ä¸‰é€£å˜")
            else:
                message=build_line_notification(pattern_no, venue_disp, race_no, time_label, time_hm, condition_text, raw_tickets, odds_hm, meta["url"], horses)
                tickets_umaban=_tickets_pop_to_umaban(raw_tickets, horses)
                bet_kind = STRATEGY_BET_KIND.get(str(pattern_no), "ä¸‰é€£å˜")

            # é€ä¿¡
            sent_ok, http_status = notify_strategy_hit_to_many(message, targets)

            # é€šçŸ¥ãƒ­ã‚°ï¼ˆé€ä¿¡æˆåŠŸæ™‚ã®ã¿ï¼‰
            if sent_ok:
                try:
                    append_notify_log({
                        'date_jst': jst_today_str(),
                        'race_id': rid,
                        'strategy': str(pattern_no),
                        'stake': len(tickets_umaban) * UNIT_STAKE_YEN,
                        'bets_json': json.dumps(tickets_umaban, ensure_ascii=False),
                        'notified_at': jst_now(),
                        'jockey_ranks': "/".join([
                            jockey_rank_letter_by_name(h.get("jockey")) if h.get("jockey") else "â€”"
                            for h in horses[:3]
                        ]),
                    })
                except Exception as e:
                    logging.exception("[WARN] append_notify_logå¤±æ•—: %s", e)

            now_epoch=time.time()
            if sent_ok:
                try:
                    sheet_upsert_notified(rid, now_epoch, note=f"{meta['venue_race']} {display_dt:%H:%M} {src_label}")
                except Exception as e:
                    logging.exception("[ERROR] TTLæ›´æ–°å¤±æ•—: %s", e)
                seen_in_this_run.add(rid)
                try:
                    ymd=now_jst().strftime("%Y%m%d")
                    sheet_append_bet_record(ymd, rid, venue_disp, race_no, pattern_no, bet_kind, tickets_umaban or [])
                except Exception as e:
                    logging.exception("[ERROR] betsè¨˜éŒ²å¤±æ•—: %s", e)
            elif http_status==429:
                try:
                    key_cd=f"{rid}:cd"; sheet_upsert_notified(key_cd, now_epoch, note=f"429 cooldown {meta['venue_race']} {display_dt:%H:%M}")
                except Exception as e:
                    logging.exception("[ERROR] CDæ›´æ–°å¤±æ•—: %s", e)

            time.sleep(random.uniform(*SLEEP_BETWEEN))

        logging.info(f"[INFO] HITS={hits} / MATCHES={matches}")

    # === æ—¥æ¬¡ã‚µãƒãƒªï¼šæŒ‡å®šæ™‚åˆ»ã«1æ—¥1å› ===
    try:
        now = now_jst()
        if _is_time_reached(now, DAILY_SUMMARY_HHMM):
            notified = {}
            try:
                notified = sheet_load_notified()
            except Exception as e:
                logging.warning("[SUMMARY] é€šçŸ¥ãƒ•ãƒ©ã‚°ã®èª­è¾¼ã«å¤±æ•—: %sï¼ˆç¶šè¡Œï¼‰", e)
            skey = _summary_key_for_today()
            if skey not in notified:
                logging.info("[SUMMARY] ãƒˆãƒªã‚¬æ™‚åˆ»åˆ°é”ï¼ˆ%sï¼‰ã€‚æœ¬æ—¥ã®ã‚µãƒãƒªã‚’é€ä¿¡ã—ã¾ã™ã€‚", DAILY_SUMMARY_HHMM)
                try:
                    summarize_today_and_notify(targets)
                except Exception as e:
                    # ã“ã“ã§æ¡ã‚Šã¤ã¶ã•ãªã„ã¨ã€Œåˆ¤å®šã«å¤±æ•—ã€ã§å…¨ä½“ãŒçµ‚äº†ã—ã¦å†é€æ©Ÿä¼šã‚’å¤±ã†
                    logging.exception("[SUMMARY] ã‚µãƒãƒªä½œæˆ/é€ä¿¡ä¸­ã«æœªæ•æ‰ä¾‹å¤–: %s", e)
                    # å¤±æ•—æ™‚ã¯ãƒ•ãƒ©ã‚°ã‚’æ›¸ã‹ãšã«æ¬¡å›ä»¥é™ã®å†è©¦è¡Œã«å§”ã­ã‚‹
                else:
                    try:
                        sheet_upsert_notified(skey, time.time(), note=f"daily summary {now:%H:%M}")
                        logging.info("[SUMMARY] ã‚µãƒãƒªé€ä¿¡ãƒ•ãƒ©ã‚°ã‚’ä¿å­˜ã—ã¾ã—ãŸï¼ˆ%sï¼‰ã€‚", skey)
                    except Exception as e:
                        logging.exception("[SUMMARY] ã‚µãƒãƒªé€ä¿¡ãƒ•ãƒ©ã‚°ã®ä¿å­˜ã«å¤±æ•—: %s", e)
            else:
                logging.info("[SUMMARY] æœ¬æ—¥ã¯æ—¢ã«ã‚µãƒãƒªé€ä¿¡æ¸ˆã¿ï¼ˆkey=%sï¼‰ã€‚ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚", skey)
    except Exception as e:
        logging.exception("[ERROR] æ—¥æ¬¡ã‚µãƒãƒªé€ä¿¡åˆ¤å®šã«å¤±æ•—: %s", e)

    logging.info("[INFO] ã‚¸ãƒ§ãƒ–çµ‚äº†")

# ========= å¸¸é§ãƒ«ãƒ¼ãƒ— =========
def run_watcher_forever(interval_sec: int = int(os.getenv("WATCHER_INTERVAL_SEC", "60"))):
    logging.info(f"[BOOT] run_watcher_forever(interval={interval_sec}s)")
    while True:
        try:
            main()
        except Exception as e:
            logging.exception("[FATAL] watcherãƒ«ãƒ¼ãƒ—ä¾‹å¤–: %s", e)
        time.sleep(interval_sec)